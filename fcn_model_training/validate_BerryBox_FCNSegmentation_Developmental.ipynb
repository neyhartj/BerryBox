{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f_7FuYyMej4V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_7FuYyMej4V",
    "outputId": "16adbd52-e4fb-419c-fc21-0b45361f7289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  8 16:46:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA Tesla V1...  Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    42W / 250W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA Tesla V1...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   42C    P0    38W / 250W |      0MiB / 16160MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# #Code Box 1\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "# # Drive folder\n",
    "# drive = \"/content/drive/MyDrive\"\n",
    "\n",
    "drive = \"/project/gifvl_vaccinium/cranberryImaging/\"\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d076506",
   "metadata": {
    "id": "4d076506"
   },
   "source": [
    "# BerryBox Image Analysis Pipeline\n",
    "\n",
    "\n",
    "## Validation only\n",
    "\n",
    "This notebalign_took provides the code to validate a developmental FCN model. **It is not meant for production use.**\n",
    "\n",
    "\n",
    "# **Materials**\n",
    "  Input the material mask name and information below. Some of the items described here may not appear below, but anything that appears below is described here.\n",
    "\n",
    "  Specifically:\n",
    " \n",
    "  **name** - The name for the material. This is pretty arbitrary, but it will be\n",
    "  used to label output folders and images.\n",
    " \n",
    "  **input_rbg_vals** - The rbg values of the material in the input mask image.\n",
    " \n",
    "  **output_val** - The greyscale value of the mask when you output the images.\n",
    "  This is arbitrary, but every material should have its own output color\n",
    "  so they can be differentiated\n",
    " \n",
    "  **confidence_threshold** - The lower this number, the more voxels will be labled a specific material. Essentially, the ML algorith outptus a confdience value  (centered on 0.5) for every voxel and every material. By default, voxels with  a confidence of 0.5 or greater are determined to be the material in question.  But we can labled voxles with a lower condience level by changing this  parameter\n",
    "\n",
    "  **training_image_directory /training_mask_directory**: Input the directory where your training images and masks are located.\n",
    "\n",
    "  **validation_fraction**: Input the fraction of images you want to validate your model during training. These are not a independent validation, but are part of the training process.\n",
    "\n",
    "  **num_models**: Enter the number of models you want to iteratively train. Because these are statistical models, the performance of any given model will vary. Training more models will allow you to select the model that best fits your data.\n",
    "  \n",
    "  **num_epochs**: Enter number of epochs that you want to use to train your model. More is generally better, but takes more time.\n",
    "\n",
    "  **batch_size**: Input your batch size. Larger batch sizes allow for faster training, but take up more VRAM. If you are running out of VRAM during training, decrease your batch size.\n",
    "\n",
    "  **scale**: Input how you want your images scaled during model training and inference. When the scale is 1, your images will be used at full size for training. When the scale is less than 1, your images will be downsized according to the scale you set for training and inference, decreasing VRAM usage. If you run out of VRAM during training, consider rescaling your images.\n",
    "  \n",
    "  **normalization_path**: The path to the normalization data file that was saved during model training.\n",
    "\n",
    "  **models_directory**: Directory where your models are saved.\n",
    "\n",
    "  **model_group**: Name for the group models you iteratively generate.\n",
    "\n",
    "  **current_model_name**: Name for each individual model you generate; will automatically be labeled 1 through n for the number of models you specify above.\n",
    "\n",
    "  **val_images/val_masks**: Input the directory where your independent validation images and masks are located. These images are not used for training and are used as an independent validation of your model.\n",
    "\n",
    "  **csv_directory**: Directory where a CSV file of your validation results will be saved.\n",
    "\n",
    "  **inference_directory**: Directory where the images you want analyzed are located.\n",
    "\n",
    "  **output_directory**: Directory where you want your analysis results to be saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "-a6Oocjd2Wu-",
   "metadata": {
    "id": "-a6Oocjd2Wu-"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Set user parameters ####\n",
    "#############################\n",
    "\n",
    "class Material:\n",
    " \n",
    "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
    "    self.name = name\n",
    "    self.input_rgb_vals = input_rgb_vals\n",
    "    self.output_val = output_val\n",
    "    self.confidence_threshold = confidence_threshold\n",
    "\n",
    "#Creating a list of materials so we can iterate through it\n",
    "materials = [\n",
    "             Material(\"background\", [0,0,0], 0, 0.5),\n",
    "             Material(\"berry\", [255,255,255], 255, 0.75),\n",
    "             ]\n",
    "\n",
    "# Project directory\n",
    "# IMPORTANT - ALL DIRECTORIES NEED TO END IN A /\n",
    "# proj_dir =  drive + \"/ARS_Cranberry/ImageAnalysis/BerryBox/fcn_model_building/\"\n",
    "proj_dir = drive + \"/BerryBox/fcn_model_building/\"\n",
    "\n",
    "num_materials = len(materials)\n",
    "\n",
    "#Decrease scale to decrease VRAM usage; if you run out of VRAM during traing, restart your runtime and down scale your images\n",
    "scale=0.3\n",
    "\n",
    "# Input model path \n",
    "# This file should end in \".pth\"\n",
    "model_path = proj_dir + \"/model_output/berryBox_fcn_0.0.2/models/berryBox_fcn_0.0.2_model3.pth\"\n",
    "\n",
    "# Normalization data path\n",
    "normalization_path = proj_dir + \"/model_output/berryBox_fcn_0.0.2/berryBox_fcn_20220608-113848_model_normalization_param.txt\"\n",
    "\n",
    "# Name of the model group\n",
    "model_group = \"berryBox_fcn_0.0.2/\"\n",
    "current_model_name = model_group.replace(\"/\", \"\") + \"_model\"\n",
    "\n",
    "# \"\"\"Hold images/annotations in reserve to test your model performance. Use this metric to decide which model you want to use \n",
    "# for your data analysis\"\"\"\n",
    "# IMPORTANT: END EACH DIRECTORY PATH WITH A \"/\"\n",
    "test_images = proj_dir + \"/test/images/\"\n",
    "test_masks = proj_dir + \"/test/masks/\"\n",
    "\n",
    "#Input the 5 alpha-numeric characters proceding the file number of your images\n",
    "  #EX. Jmic3111_S0_GRID image_0.tif ----->mage_\n",
    "proceeding=\"image\"\n",
    "#Input the 4 or more alpha-numeric characters following the file number\n",
    "  #EX. Jmic3111_S0_GRID image_0.tif ----->.tif\n",
    "following=\".png\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38QCpWHfst0y",
   "metadata": {
    "id": "38QCpWHfst0y",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Randomize training and testing images\n",
    "\n",
    "Run this block to randomize the training and testing images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AEDGd_m2s1rL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEDGd_m2s1rL",
    "outputId": "c4d2cdc8-3269-48fa-b531-d1e31566b7e0"
   },
   "outputs": [],
   "source": [
    "# Directory containing all images\n",
    "all_image_directory = proj_dir + \"/annotatedImagesMask\"\n",
    "# Fraction of images to use for testing\n",
    "testing_fraction = 0.25\n",
    "# Image suffix\n",
    "image_suffix = \"-corrected.PNG\"\n",
    "# Mask suffix\n",
    "mask_suffix = \"-berry_mask_cleaned.PNG\"\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "import os\n",
    "from random import sample\n",
    "from shutil import copyfile\n",
    "\n",
    "# How many images total??\n",
    "image_files = [x for x in os.listdir(all_image_directory) if image_suffix in x]\n",
    "print(str(len(image_files)) + \" image files detected.\")\n",
    "\n",
    "# Create the training and testing directories; if they already exist, clear them\n",
    "# and resample\n",
    "dirs_create = [training_image_directory, training_mask_directory, test_images, test_masks]\n",
    "for dirname in dirs_create:\n",
    "  if os.path.exists(dirname):\n",
    "    dir_contents = os.listdir(dirname)\n",
    "    dir_contents = [dirname + \"/\" + x for x in dir_contents]\n",
    "    for filename in dir_contents:\n",
    "      os.remove(filename)\n",
    "  else:\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "\n",
    "# List all images in the directory\n",
    "all_images_masks = os.listdir(all_image_directory)\n",
    "# Find all of the images\n",
    "all_images = [x for x in all_images_masks if image_suffix in x]\n",
    "all_masks = [x for x in all_images_masks if mask_suffix in x]\n",
    "# Remove the suffixes\n",
    "all_prefixes = [x.replace(image_suffix, \"\") for x in all_images]\n",
    "\n",
    "# Randomly select prefixes for testing\n",
    "testing_prefixes = sample(all_prefixes, int(testing_fraction * len(all_prefixes)))\n",
    "# The remaining prefixes are for training\n",
    "training_prefixes = [x for x in all_prefixes if x not in testing_prefixes]\n",
    "\n",
    "# Iterate over all prefixes\n",
    "for i, prefix in enumerate(all_prefixes):\n",
    "  image_number = str(i + 1).zfill(len(str(len(all_prefixes))))\n",
    "\n",
    "  # Find the image and the mask associated with this prefix\n",
    "  prefix_i_image = [x for x in all_images if prefix in x][0]\n",
    "  prefix_i_mask = [x for x in all_masks if prefix in x][0]\n",
    "\n",
    "  # Create a file basename to save\n",
    "  prefix_basename = prefix + \"image\" + image_number + following\n",
    "\n",
    "  # Determine where to put the file\n",
    "  if prefix in training_prefixes:\n",
    "    copyfile(all_image_directory + \"/\" + prefix_i_image, training_image_directory + \"/\" + prefix_basename)\n",
    "    copyfile(all_image_directory + \"/\" + prefix_i_mask, training_mask_directory + \"/\" + prefix_basename)\n",
    "  else:\n",
    "    copyfile(all_image_directory + \"/\" + prefix_i_image, test_images + \"/\" + prefix_basename)\n",
    "    copyfile(all_image_directory + \"/\" + prefix_i_mask, test_masks + \"/\" + prefix_basename)    \n",
    "\n",
    "\n",
    "print(str(len(os.listdir(training_image_directory))) + \" images were assigned to the training set.\")\n",
    "print(str(len(os.listdir(test_images))) + \" images were assigned to the testing set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80899b8d-4327-4898-81b2-9a8bd430302c",
   "metadata": {
    "id": "80899b8d-4327-4898-81b2-9a8bd430302c",
    "tags": []
   },
   "source": [
    "# Load functions\n",
    "\n",
    "Do not skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97d004e-f1c6-4a55-b879-8cc3bcc8f39a",
   "metadata": {
    "id": "f97d004e-f1c6-4a55-b879-8cc3bcc8f39a"
   },
   "outputs": [],
   "source": [
    "\n",
    "#############################\n",
    "#### Function  loading   ####\n",
    "#############################\n",
    "\n",
    "\n",
    "# This is the directory where models and their output will be saved\n",
    "models_directory = proj_dir + \"/model_output/\"\n",
    "# Directory for the model group (this will store the CSV output and the normalization data)\n",
    "model_group_directory = models_directory + \"/\" + model_group\n",
    "# Directory for the model group / outputs\n",
    "output_directory = model_group_directory + \"/models/\"\n",
    "# Directory for outputing segmented images from the testing set\n",
    "seg_output_directory = model_group_directory + \"/segmented_images/\"\n",
    "# Name of the CSV file to create\n",
    "csv_directory =  model_group_directory + model_group.replace(\"/\", \"\") + \".csv\"\n",
    "\n",
    "\n",
    "# pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html    \n",
    "\n",
    "\n",
    "# #**Parameter Loading**\n",
    "\n",
    "#Code Box \n",
    "import os\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from PIL import Image\n",
    "import random\n",
    "#import scipy.ndimage as ndi\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.ndimage import morphology\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "# Create a list of directories to check and create\n",
    "for dirpath in [models_directory, model_group_directory, output_directory, seg_output_directory]:\n",
    "    # Check if exists\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "    \n",
    "class BasicDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, scale=scale, transform=False):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.scale = scale\n",
    "        self.transform=transform\n",
    "        self.t_list=A.Compose([A.HorizontalFlip(p=0.4),A.VerticalFlip(p=0.4), A.Rotate(limit=(-50, 50), p=0.4),])\n",
    "        self.means=[0]\n",
    "        self.stds=[1]\n",
    "\n",
    "        \n",
    "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
    " \n",
    "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
    "                    if not file.startswith('.')]\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    " \n",
    " \n",
    "    @classmethod\n",
    "    def mask_preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    " \n",
    "        img_nd = np.array(pil_img)\n",
    " \n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    " \n",
    "       \n",
    "        return img_nd\n",
    "    \n",
    " \n",
    "        \n",
    "    def img_preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))\n",
    " \n",
    "        img_nd = np.array(pil_img)\n",
    " \n",
    "        if len(img_nd.shape) == 2:\n",
    "            img_nd = np.expand_dims(img_nd, axis=2)\n",
    " \n",
    "       \n",
    " \n",
    "        return img_nd\n",
    " \n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        mask_file = glob(self.masks_dir + idx + '*')\n",
    "        img_file = glob(self.imgs_dir + idx + '*')\n",
    " \n",
    "        assert len(mask_file) == 1,             f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
    "        assert len(img_file) == 1,             f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        mask = Image.open(mask_file[0])\n",
    "        img = Image.open(img_file[0])\n",
    " \n",
    "  \n",
    "        \n",
    " \n",
    "        \n",
    "        #Reshapes from 1 channel to 3 channels in grayscale\n",
    "        img = self.img_preprocess(img, self.scale)\n",
    "        mask = self.mask_preprocess(mask, self.scale)\n",
    "        # new_image=np.zeros((img.shape[0],img.shape[1],3))\n",
    "        # new_image[:,:,0]=img[:,:,0]\n",
    "        # new_image[:,:,1]=img[:,:,0]\n",
    "        # new_image[:,:,2]=img[:,:,0]\n",
    "        \n",
    " \n",
    " \n",
    " \n",
    "        img=img\n",
    " \n",
    "        new_mask = np.zeros((num_materials,img.shape[0],img.shape[1]))\n",
    "        # print(mask.shape)       \n",
    "        for i, mat in enumerate(materials):\n",
    "          # plt.imshow(mask[:,:,0])\n",
    "          # plt.show()\n",
    "          indices = np.all(mask == mat.input_rgb_vals, axis=-1)\n",
    "          new_mask[i,:,:][indices] = 1\n",
    " \n",
    "        mask = new_mask\n",
    "  \n",
    "        # plt.imshow(mask[1,:,:])\n",
    "        # i=6\n",
    "        # for i in range(len(mask)):\n",
    "        #   plt.imshow(mask[i,:,:])\n",
    "        #   plt.show()\n",
    "        \n",
    "        if img.max() > 1:\n",
    "            img = img / 255\n",
    " \n",
    "       \n",
    " \n",
    "        \n",
    "        if self.transform:\n",
    "            augmented=self.t_list(image=img, masks=mask)\n",
    "            img=augmented[\"image\"]\n",
    "            mask=augmented[\"masks\"]\n",
    "            \n",
    " \n",
    "        \n",
    " \n",
    "        img = img.transpose((2, 0, 1))\n",
    "        \n",
    "        mask=np.array(mask)\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    " \n",
    "        img=torch.from_numpy(img)\n",
    "        mask=torch.from_numpy(mask)\n",
    "        \n",
    "        img=transforms.Normalize(mean=self.means, std=self.stds)(img)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OaEzTdfG-WuR",
   "metadata": {
    "id": "OaEzTdfG-WuR",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Parameter loading\n",
    "\n",
    "This block can be skipped if you are only interested in model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kksCPMtI-TPi",
   "metadata": {
    "id": "kksCPMtI-TPi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#############################\n",
    "#### Parameter  loading   ####\n",
    "#############################\n",
    " \n",
    "        \n",
    "dataset = BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=False)\n",
    " \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!Set batch size here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# train, val=trainval_split(dataset, val_fraction=0.5)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    "#val_loader = DataLoader(val, batch_size=3, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    "nimages = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for batch, _ in train_loader:\n",
    "    # Rearrange batch to be the shape of [B, C, W * H]\n",
    "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "    # Update total number of images\n",
    "    nimages += batch.size(0)\n",
    "    # Compute mean and std here\n",
    "    mean += batch.mean(2).sum(0) \n",
    "    std += batch.std(2).sum(0)\n",
    " \n",
    "# Final step\n",
    "mean /= nimages\n",
    "std /= nimages\n",
    " \n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "dataset.means=mean\n",
    "dataset.stds=std \n",
    "\n",
    "nimages = 0\n",
    "newmean = 0.\n",
    "newstd = 0.\n",
    "for batch, _ in train_loader:\n",
    "    # Rearrange batch to be the shape of [B, C, W * H]\n",
    "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "    # Update total number of images\n",
    "    nimages += batch.size(0)\n",
    "    # Compute mean and std here\n",
    "    newmean += batch.mean(2).sum(0) \n",
    "    newstd += batch.std(2).sum(0)\n",
    " \n",
    "# Final step\n",
    "newmean /= nimages\n",
    "newstd /= nimages\n",
    " \n",
    "print(newmean)\n",
    "print(newstd)\n",
    "\n",
    "\n",
    "## Save these normalization values for the production pipeline ##\n",
    "\n",
    "# Open a file\n",
    "param_filename = model_group_directory + model_group.replace(\"/\", \"\") + \"_normalization_param.txt\"\n",
    "handle = open(param_filename, \"w\")\n",
    "\n",
    "# Write all the parameters to this file\n",
    "handle.write(\"normalization_mean\" + \"\\t\" + str(newmean) + \"\\n\")\n",
    "handle.write(\"normalization_std\" + \"\\t\" + str(newstd) + \"\\n\")\n",
    "\n",
    "# Close the file\n",
    "handle.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ee51d-9131-440b-99fc-9b2eac68f3e8",
   "metadata": {
    "id": "235ee51d-9131-440b-99fc-9b2eac68f3e8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61086587-71ca-4a0e-9777-76a3548997ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "id": "61086587-71ca-4a0e-9777-76a3548997ca",
    "outputId": "d4ba72ab-388e-4b5a-d6be-8bb3fe946994"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Model training   #######\n",
    "#############################\n",
    "\n",
    "# #**Model** **Training**\n",
    "# Please do not alter this code.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "## Proceed with model training only if the indicator above is True\n",
    "if new_training:\n",
    "\n",
    "  #For loop for FCN model training Cell Code Box 4\n",
    "  #!cd \"drive/My Drive/Colab Notebooks\"\n",
    "  # Semantic Segmentation and Data Extraction in Pytorch Using FCN by Pranav Raja and a tiny bit by Devin Rippner (Plant AI and BioPhysics Lab)\n",
    "  # a work in progress, works well overall but need mroe people to look at it and identify bugs\n",
    "  #%%\n",
    "   \n",
    "  import torchvision\n",
    "  from torchvision.models.segmentation.fcn import FCNHead\n",
    "  from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "  from torch.utils.data import DataLoader, random_split\n",
    "  import torch\n",
    "  # from torch._six import container_abcs, string_classes, int_classes\n",
    "  import torchvision.transforms as T\n",
    "  import matplotlib.pyplot as plt\n",
    "  import torch.nn as nn\n",
    "  import os\n",
    "  import psutil\n",
    "  import gc\n",
    "  import pandas as pd\n",
    "  import random\n",
    "   \n",
    "  dir_checkpoint = output_directory\n",
    "   \n",
    "   \n",
    "  model_group=model_group\n",
    "  # List the models in 'output_directory'\n",
    "  n_output_models = len([x for x in os.listdir(output_directory) if current_model_name in x])\n",
    "  num_models_range = [x for x in range(n_output_models, num_models + n_output_models)] \n",
    "  # num_models_range=range(num_models)\n",
    "#   # Create model group and name paths\n",
    "#   model_group_dir1 = os.path.join(dir_checkpoint, model_group)\n",
    "#   if not os.path.exists(model_group_dir1):\n",
    "#     os.mkdir(model_group_dir1)\n",
    "  \n",
    "  seed=0\n",
    "  torch.manual_seed(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  for i in num_models_range:\n",
    "    #!!!!!!! Here we pull in a pretrained FCN on torch and we replace the output layer since we have six classes rather than 21!!!!!!!!\n",
    "    num_classes=num_materials\n",
    "    model=torchvision.models.segmentation.fcn_resnet101(pretrained=True, progress=True)\n",
    "    # model.backbone.conv1=nn.Sequential(nn.Conv2d(1,3, (1,1), (1,1), (0,0), bias=False), model.backbone.conv1)\n",
    "    model.classifier=FCNHead(2048, num_classes)\n",
    "    \n",
    "    def trainval_split(dataset, val_fraction=0.5):\n",
    "   \n",
    "      validation_size = int(len(dataset) * val_fraction)\n",
    "      train_size = len(dataset) - validation_size\n",
    "      # print(validation_size)\n",
    "      # print(train_size)\n",
    "      # print(len(dataset))\n",
    "      # print(dataset.dataset_size)\n",
    "      train, val = torch.utils.data.random_split(dataset, [train_size, validation_size], generator=torch.Generator().manual_seed(i))\n",
    "   \n",
    "      return train, val\n",
    "   \n",
    "   \n",
    "   \n",
    "    dataset= BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=True)\n",
    "    dataset_train, dataset_val=trainval_split(dataset, val_fraction=validation_fraction)\n",
    "    #!!!!!select folders for the images and masks associated with training and validation here. Also specify image scaling factor here!!!!!!!!!!!!!!!!\n",
    "    # dataset_train = BasicDataset(training_image_directory, training_mask_directory, 1, transform=True)\n",
    "    # dataset_val = BasicDataset(training_image_directory, training_mask_directory, 1, transform=False)\n",
    "    # dataset_train = BasicDataset(\"drive/My Drive/FCN WORKFLOW PAPER/train/image_/\", \"drive/My Drive/FCN WORKFLOW PAPER/train/mask_edited2/\", 1, transform=True)\n",
    "    # dataset_val = BasicDataset(\"drive/My Drive/FCN WORKFLOW PAPER/test/image_/\", \"drive/My Drive/FCN WORKFLOW PAPER/test/mask_edited2/\", 1, transform=False)\n",
    "    \n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Specify Batch Size Here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)#, collate_fn=pad_collate)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    "   \n",
    "   \n",
    "    #%%\n",
    "   \n",
    "    # this is the train code \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!! Input epochs here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    num_epochs=num_epochs\n",
    "    # read up on optimizers but Adam should work for now, if you get good results with Adam then you can try SGD (it's harder to tune but usually converges better)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "   \n",
    "    #just initializing a value called best_loss\n",
    "    best_loss=999\n",
    "    \n",
    "    # Lists to save the training/test loss\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "   \n",
    "    # choose a loss function\n",
    "    # criterion=nn.CrossEntropyLoss()\n",
    "    #criterion=nn.BCELoss().cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # class diceloss(nn.Module):\n",
    "    #     def __init__(self, epsilon):\n",
    "    #         # super(diceloss, self).init()\n",
    "    #         super(diceloss, self).__init__()\n",
    "    #         self.sigmoid=nn.Sigmoid()\n",
    "    #         self.epsilon=epsilon\n",
    "    #         # print('HI')\n",
    "    #     def forward(self, pred, target):\n",
    "    #         if target.size() != pred.size():\n",
    "    #             raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), pred.size()))\n",
    "    #         pred=self.sigmoid(pred)\n",
    "    #         tp=torch.sum(target*pred, (1,2,3))\n",
    "    #         fp=torch.sum((1-target)*pred, (1,2,3))\n",
    "    #         fn=torch.sum(target*(1-pred), (1,2,3))\n",
    "    #         # precision=tp/(tp+fp)\n",
    "    #         # recall=tp/(tp+fn)\n",
    "    #         f1=(tp)/(tp+self.epsilon+0.5*(fp+fn))\n",
    "    #         # print(f1)\n",
    "    #         return 1-torch.mean(f1)\n",
    "    # criterion=diceloss(epsilon=epsilon)\n",
    "    #\n",
    "    # model.train()\n",
    "    # model.train()\n",
    "    # \n",
    "    #this is the train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(psutil.virtual_memory().percent)\n",
    "        print('Epoch: ', str(epoch))\n",
    "      #add back if doing fractional training\n",
    "        train_loader.dataset.dataset.transform=True\n",
    "        model.train()\n",
    "        \n",
    "        # List to store loss for this training epoch\n",
    "        current_train_loss = []\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "   \n",
    "            images = images.to(device=device, dtype=torch.float32)\n",
    "            masks = masks.to(device=device, dtype=torch.float32)\n",
    "   \n",
    "            #forward pass\n",
    "            preds=model(images)['out'].cuda()\n",
    "          \n",
    "            #compute loss\n",
    "            loss=criterion(preds, masks)\n",
    "          \n",
    "            #reset the optimizer gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "   \n",
    "            #backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "   \n",
    "            #use the computed gradients to update model weights\n",
    "            optimizer.step()\n",
    "        \n",
    "            # add the loss current_train_loss\n",
    "            current_train_loss.append(loss.to('cpu').detach())\n",
    "   \n",
    "            # print('Train loss: '+str(loss.to('cpu').detach()))\n",
    "        # model.eval()\n",
    "        #add back if doing fractional training\n",
    "        val_loader.dataset.dataset.transform=False\n",
    "        current_loss=0\n",
    "        current_test_loss = []\n",
    "        \n",
    "        #test on val set and save the best checkpoint\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "          for images, masks in val_loader:\n",
    "              images = images.to(device=device, dtype=torch.float32)\n",
    "              masks = masks.to(device=device, dtype=torch.float32)\n",
    "              preds=model(images)['out'].cuda()\n",
    "              # print(preds)\n",
    "              # print(masks)\n",
    "              loss=criterion(preds, masks)\n",
    "              #print('hi')\n",
    "              loss_save = loss.to('cpu').detach()\n",
    "              current_loss+=loss_save\n",
    "              current_test_loss.append(loss_save)\n",
    "              del images, masks, preds, loss\n",
    "    #!!!!!!!!!!!!!!!!!!!!!!!!!!!Re-name model here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!        \n",
    "        if best_loss>current_loss:\n",
    "            best_loss=current_loss\n",
    "            print('Best Model Saved!, loss: '+ str(best_loss))\n",
    "            torch.save(model.state_dict(), dir_checkpoint + \"/\" + current_model_name+str(i+1)+\".pth\")\n",
    "        else:\n",
    "            print('Model is bad!, Current loss: '+ str(current_loss) + ' Best loss: '+str(best_loss))\n",
    "        print('\\n')\n",
    "        \n",
    "        # Save the average train loss and the test loss\n",
    "        train_loss.append(np.mean(np.asarray(current_train_loss)))\n",
    "        test_loss.append(np.mean(np.asarray(current_test_loss)))\n",
    "        \n",
    "    ## Save the loss data ##\n",
    "    \n",
    "    loss_data = {\"epoch\" : [i+1 for i in range(0, len(train_loss))],\n",
    "                 \"train_loss\" : train_loss,\n",
    "                 \"test_loss\" : test_loss}\n",
    "    loss_df = pd.DataFrame(loss_data, columns = [\"epoch\", \"train_loss\", \"test_loss\"])\n",
    "    loss_df.to_csv(model_group_directory + current_model_name + \"_loss_tracking.csv\")\n",
    "        \n",
    "        \n",
    "\n",
    "# If no retraining, adjust the current model name to reflect the most recent models\n",
    "else:\n",
    "    import re\n",
    "    import os\n",
    "    \n",
    "    models_dir = output_directory\n",
    "    # List the trained models\n",
    "    trained_models = [x for x in os.listdir(models_dir) if \".pth\" in x]\n",
    "    # Cut the #.pth from the end of each file; find the unique names\n",
    "    unique_model_name = list(set([re.sub(\"\\d+.pth\", \"\", x) for x in trained_models]))\n",
    "    # Sort\n",
    "    unique_model_name.sort()\n",
    "    # Get the last item (this is the most recent)\n",
    "    current_model_name = unique_model_name[-1]\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0818b4-b01e-4a72-90d2-81381beb7b0c",
   "metadata": {
    "id": "0a0818b4-b01e-4a72-90d2-81381beb7b0c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8654cc0f-022d-4376-947e-8f0e9719b8fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "ce3d25a594a340d0b01caf26ad9cbd3a",
      "282f7540606c4689b6e37fe84f6dc3a5",
      "9773ef196c7249009882912e3969490b",
      "1e10c23628f34108b2c07936128358c0",
      "e0478ba3833444ed9e865f91abfd9ee0",
      "785bf005029d4fd3bf7828b8abfd97f2",
      "96bfcf21175e47ecb5f95d020d561dd6",
      "64fb06d099564c6cac73014f0f9413ed",
      "cda3514e741b41b0b15a1601bc7f3105",
      "3c689f1e14da4710a0f000406718f7bf",
      "4a334a60da00453eba099fa645e31bec"
     ]
    },
    "id": "8654cc0f-022d-4376-947e-8f0e9719b8fe",
    "outputId": "cc0cf20c-8b6f-4697-fceb-4f17f23a2ba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berryBox_fcn_0.0.2_model1.pth\n",
      "berryBox_fcn_0.0.2_model3.pth\n",
      "berryBox_fcn_0.0.2_model2.pth\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "#### Model validation #######\n",
    "#############################\n",
    "\n",
    "\"\"\"# **Validation**\n",
    "Please do not alter this code\n",
    "\"\"\"\n",
    "\n",
    "# Recalculate the number of models\n",
    "import os\n",
    "models_dir = output_directory\n",
    "model_list = [x for x in os.listdir(models_dir) if current_model_name in x]\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "#from statistics import mean\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# interim_list=[]\n",
    "modeldata = pd.DataFrame(columns=[\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
    "# for mat in enumerate(materials):\n",
    "#   modeldata=pd.DataFrame(columns=['model_group',mat.name + \" precision\",mat.name + \" recall\",mat.name + \" accuracy\",mat.name + \" f1\"])\n",
    "\n",
    " \n",
    "for model_name_i in model_list:\n",
    "  # model=torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True, progress=True)\n",
    " \n",
    " \n",
    "  model=torchvision.models.segmentation.fcn_resnet101(pretrained=False)\n",
    "  # model.backbone.conv1=nn.Sequential(nn.Conv2d(1,3, (1,1), (1,1), (0,0), bias=False), model.backbone.conv1)\n",
    "  #!!!!!!!!!!!!!!!!!Specify Layer # here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  model.classifier=FCNHead(2048, num_materials)\n",
    "  # model.classifier=DeepLabHead(2048, 6)\n",
    "  device = torch.device('cuda')\n",
    " \n",
    "  outputs=[]\n",
    "  model.to(device)\n",
    " \n",
    "  #!!!!!!!!!!!!!!!!!!!!!Select Correct Model from the best models directory!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    " \n",
    "  model.load_state_dict(torch.load(output_directory + \"/\" + model_name_i), strict=False)\n",
    " \n",
    " \n",
    "  model.train()\n",
    " \n",
    "  dataset_val = BasicDataset(test_images, test_masks, scale=scale, transform=False)\n",
    "  val_loader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    " \n",
    "  prop_list = []\n",
    "  for mat in materials:\n",
    "    prop_list.append([[],[],[],[]])\n",
    " \n",
    "  for images, target in val_loader:\n",
    "    images = images.to(device=device, dtype=torch.float32)\n",
    "    target = target.to(device=device, dtype=torch.float32)\n",
    " \n",
    "    with torch.no_grad():\n",
    "      pred=model(images)['out'].cuda()\n",
    "      pred=nn.Sigmoid()(pred)\n",
    "    \n",
    "    for i, mat in enumerate(materials):\n",
    "      \n",
    "#       # Skip background\n",
    "#       if mat.name == \"background\":\n",
    "#         continue\n",
    "        \n",
    "      material_target=target[:,i,:,:]\n",
    "      material_pred = pred[:, i, :, :]\n",
    "      material_pred[material_pred >=mat.confidence_threshold] = 1\n",
    "      material_pred[material_pred <=mat.confidence_threshold] = 0\n",
    "      pred[:, i, :, :]=material_pred\n",
    " \n",
    "      material_tp=torch.sum(material_target*material_pred, (1,2))\n",
    "      material_fp=torch.sum((1-material_target)*material_pred, (1,2))\n",
    "      material_fn=torch.sum(material_target*(1-material_pred), (1,2))\n",
    "      material_tn=torch.sum((1-material_target)*(1-material_pred), (1,2))\n",
    " \n",
    "      material_precision=torch.mean(material_tp/(material_tp+material_fp))\n",
    "      material_recall=torch.mean(material_tp/(material_tp+material_fn))\n",
    "      material_accuracy=torch.mean((material_tp+material_tn)/(material_tp+material_tn+material_fp+material_fn))\n",
    "      material_f1=torch.mean((material_tp)/(material_tp+0.5*(material_fp+material_fn)))\n",
    " \n",
    "    \n",
    "      prop_list[i][0].append(material_precision.cpu().detach().numpy())\n",
    "      prop_list[i][1].append(material_recall.cpu().detach().numpy())\n",
    "      prop_list[i][2].append(material_accuracy.cpu().detach().numpy())\n",
    "      prop_list[i][3].append(material_f1.cpu().detach().numpy())\n",
    " \n",
    "          \n",
    " \n",
    " \n",
    " \n",
    "  # print(current_model_name+str(s+1))\n",
    "  model_name=current_model_name\n",
    "  # model_number=(str(s+1))\n",
    "  model_number = model_name_i.replace(\".pth\", \"\").replace(current_model_name, \"\")\n",
    "  print(model_name_i)\n",
    " \n",
    "  #printing with pandas\n",
    "  properties = {\"name\" : [mat.name for mat in materials],\n",
    "                \"precision\" : [str(np.nanmean(prop_list[i][0])) for i in range(num_materials)],\n",
    "                \"recall\" : [str(np.nanmean(prop_list[i][1])) for i in range(num_materials)],\n",
    "                \"accuracy\" : [str(np.nanmean(prop_list[i][2])) for i in range(num_materials)],\n",
    "                \"f1\" : [str(np.nanmean(prop_list[i][3])) for i in range(num_materials)]}\n",
    "  df = pd.DataFrame(properties, columns = [\"name\", \"precision\", \"recall\", \"accuracy\", \"f1\"])\n",
    "  df=pd.concat([df, pd.DataFrame(columns=[\"model number\",\"model name\"])])\n",
    "  df[[\"model number\",\"model name\"]]=[model_number, model_name_i]\n",
    "  # display(df)\n",
    "  \n",
    "  modeldata=modeldata.append([df], ignore_index=True)\n",
    "\n",
    "\n",
    " \n",
    "#   for i, mat in enumerate(materials):\n",
    "#     precision_final = np.mean(prop_list[i][0])\n",
    "#     print(mat.name + \" precision: \" + str(precision_final))\n",
    "#     recall_final = np.mean(prop_list[i][1])\n",
    "#     print(mat.name + \" recall: \" + str(recall_final))\n",
    "#     accuracy_final = np.mean(prop_list[i][2])\n",
    "#     print(mat.name + \" accuracy: \" + str(accuracy_final))\n",
    "#     f1_final = np.mean(prop_list[i][3])\n",
    "#     print(mat.name + \" f1: \" + str(f1_final))\n",
    "#     # modeldata1=modeldata.append({'name': mat.name, mat.name + \" precision\": precision_final, mat.name + \" recall\": recall_final, mat.name + \" accuracy\": accuracy_final, mat.name + \" f1\": f1_final}, ignore_index=True)\n",
    "#     # model_data=modeldata.append(modeldata1)\n",
    "# # model_data=df.append(interim_list)\n",
    "# print(modeldata)\n",
    "# md=pd.concat([modeldata, pd.DataFrame(columns=[\"model name\"])])\n",
    "# md[[\"model name\"]]=[current_model_name]\n",
    "# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# display(modeldata)\n",
    "\n",
    "#   display(modeldata)\n",
    "\n",
    "\"\"\"#**Save Validation CSV**\n",
    "Please do not alter this code\n",
    "\"\"\"\n",
    "# display(modeldata)\n",
    "modeldata.to_csv(csv_directory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e854c9d-a757-4e3e-b0ca-977772e24175",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "7e854c9d-a757-4e3e-b0ca-977772e24175",
    "outputId": "4324a721-65b7-4c02-d6ad-0ac711cb3945"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>model number</th>\n",
       "      <th>model name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>background</td>\n",
       "      <td>0.9968532</td>\n",
       "      <td>0.993305</td>\n",
       "      <td>0.9917157</td>\n",
       "      <td>0.99507266</td>\n",
       "      <td>1</td>\n",
       "      <td>berryBox_fcn_0.0.2_model1.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.9850191</td>\n",
       "      <td>0.95904</td>\n",
       "      <td>0.99177915</td>\n",
       "      <td>0.97178966</td>\n",
       "      <td>1</td>\n",
       "      <td>berryBox_fcn_0.0.2_model1.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>background</td>\n",
       "      <td>0.9970807</td>\n",
       "      <td>0.99353504</td>\n",
       "      <td>0.99210453</td>\n",
       "      <td>0.995302</td>\n",
       "      <td>3</td>\n",
       "      <td>berryBox_fcn_0.0.2_model3.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.9849602</td>\n",
       "      <td>0.96185225</td>\n",
       "      <td>0.99219674</td>\n",
       "      <td>0.9732146</td>\n",
       "      <td>3</td>\n",
       "      <td>berryBox_fcn_0.0.2_model3.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>background</td>\n",
       "      <td>0.99602157</td>\n",
       "      <td>0.99218816</td>\n",
       "      <td>0.9900805</td>\n",
       "      <td>0.9940959</td>\n",
       "      <td>2</td>\n",
       "      <td>berryBox_fcn_0.0.2_model2.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>berry</td>\n",
       "      <td>0.98205185</td>\n",
       "      <td>0.95117205</td>\n",
       "      <td>0.99018204</td>\n",
       "      <td>0.9662628</td>\n",
       "      <td>2</td>\n",
       "      <td>berryBox_fcn_0.0.2_model2.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name   precision      recall    accuracy          f1 model number  \\\n",
       "0  background   0.9968532    0.993305   0.9917157  0.99507266            1   \n",
       "1       berry   0.9850191     0.95904  0.99177915  0.97178966            1   \n",
       "2  background   0.9970807  0.99353504  0.99210453    0.995302            3   \n",
       "3       berry   0.9849602  0.96185225  0.99219674   0.9732146            3   \n",
       "4  background  0.99602157  0.99218816   0.9900805   0.9940959            2   \n",
       "5       berry  0.98205185  0.95117205  0.99018204   0.9662628            2   \n",
       "\n",
       "                      model name  \n",
       "0  berryBox_fcn_0.0.2_model1.pth  \n",
       "1  berryBox_fcn_0.0.2_model1.pth  \n",
       "2  berryBox_fcn_0.0.2_model3.pth  \n",
       "3  berryBox_fcn_0.0.2_model3.pth  \n",
       "4  berryBox_fcn_0.0.2_model2.pth  \n",
       "5  berryBox_fcn_0.0.2_model2.pth  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X49s7N2WXHKM",
   "metadata": {
    "id": "X49s7N2WXHKM"
   },
   "source": [
    "# **Image Segmentation**\n",
    "\n",
    "Run image segmentation on the test images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-MuczokV7w-A",
   "metadata": {
    "id": "-MuczokV7w-A"
   },
   "source": [
    "## Load a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nPEGUMpy7zp_",
   "metadata": {
    "id": "nPEGUMpy7zp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Choose the material names for which to save images\n",
    "materials_toprint = [\"berry\"]\n",
    "\n",
    "model=torchvision.models.segmentation.fcn_resnet101(pretrained=False)\n",
    "# model.backbone.conv1=nn.Sequential(nn.Conv2d(1,3, (1,1), (1,1), (0,0), bias=False), model.backbone.conv1)\n",
    "\n",
    "model.classifier=FCNHead(2048, num_materials)\n",
    " \n",
    "device = torch.device('cuda')\n",
    " \n",
    "outputs=[]\n",
    "model.to(device)\n",
    " \n",
    "#!!!!!!!!!!!!!!!!!!!!!Select Correct Model from the best models directory!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    "#model.load_state_dict(torch.load('drive/My Drive/Mina_Colab_Notebook/best_models/500epoch3train3test486_R2_Sep.pth'), strict=False)\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7qVnO9B8IK-",
   "metadata": {
    "id": "S7qVnO9B8IK-"
   },
   "source": [
    "## Run image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "AI-C1VYQ7UgR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AI-C1VYQ7UgR",
    "outputId": "d0901856-3bfa-42c0-f958-cf000559a71e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: rgb2grey is deprecated. It will be removed in version 0.19.Please use rgb2gray instead.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_1076image02_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0274image07_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0396image12_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0534image17_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0611image20_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0600image22_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0306image23_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0588image25_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0277image29_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/project/gifvl_vaccinium/cranberryImaging/virtenv_cuda113/lib/python3.6/site-packages/ipykernel_launcher.py:125: UserWarning: /project/gifvl_vaccinium/cranberryImaging//BerryBox/fcn_model_building//model_output//berryBox_fcn_0.0.2//segmented_images//background/DSC_0524image30_background_mask.png is a low contrast image\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float32 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Packages to load\n",
    "from skimage.color import rgb2grey, label2rgb\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import io,exposure, feature, filters, io, measure, morphology, restoration, segmentation, util, data\n",
    "import torchvision.transforms as T\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!Put the name of the folder with the images you want to analyze here!!!!!!!!!!!!!!!!!!!!!!!\n",
    "dir_name = test_images\n",
    "filenames = os.listdir(dir_name)\n",
    "print(str(len(filenames)) + \" images found.\")\n",
    "\n",
    "# Empty dict to store tensors\n",
    "norm_tensors = {}\n",
    "\n",
    "# Read in the normalization file\n",
    "with open(normalization_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        tabs = line.split(\"\\t\")\n",
    "        \n",
    "        # Create a vector of numeric characters\n",
    "        num_char_vec = tabs[1].split(\"[\")[1].split(\"]\")[0].split(\", \")\n",
    "        # Convert this to numeric\n",
    "        num_vec = [float(x) for x in num_char_vec]\n",
    "        # Convert to np array\n",
    "        num_arr = np.array(num_vec)\n",
    "        \n",
    "        # Convert to tensor and store\n",
    "        norm_tensors[tabs[0]] = torch.tensor(num_arr)\n",
    "        \n",
    "# assign to mean and std\n",
    "mean = norm_tensors['normalization_mean']\n",
    "std = norm_tensors['normalization_std']\n",
    "\n",
    "sort_idx = np.argsort([(int(filename.split(proceeding)[1].split(following)[0])) for filename in filenames])\n",
    "# sort_idx = np.argsort([(int(filename.split(following)[0])) for filename in filenames])\n",
    "\n",
    "for i in sort_idx:\n",
    "    #makes new directory called \"(directory name here) + name in red\" that your new images go into\n",
    "    new_dir_name = seg_output_directory\n",
    "    if not os.path.exists(new_dir_name):\n",
    "      os.makedirs(new_dir_name)\n",
    "    \n",
    "    for mat in materials:\n",
    "      new_dir_name_mat= new_dir_name + \"/\" + mat.name\n",
    "      if not os.path.exists(new_dir_name_mat):\n",
    "        os.makedirs(new_dir_name_mat)\n",
    "    filename = filenames[i]\n",
    "    \n",
    "    image = Image.open(dir_name +'/'+ filenames[i])\n",
    "    image1=(image)\n",
    "    image1=np.array(image1)\n",
    "    image1=rgb2grey(image1)\n",
    "    image1 = rescale(image1, scale, anti_aliasing=True)\n",
    "    \n",
    "    w, h = image.size\n",
    "    # print(image.size)\n",
    "    #!!!!!!!!!!!!!!!!!!!!Make sure scale matches!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    scale=scale\n",
    "    newW, newH = int(scale * w), int(scale * h)\n",
    "    image=image.resize((newW, newH))\n",
    "    image1 = image1[0:newH, 0:newW] # Add this to resize image1\n",
    "    image=np.array(image, dtype=float)\n",
    "    new_im=np.zeros((3, newH, newW))\n",
    "    new_im[0,:,:]=image[:,:,0]\n",
    "    new_im[1,:,:]=image[:,:,1]\n",
    "    new_im[2,:,:]=image[:,:,2]\n",
    "    image=new_im\n",
    "    \n",
    "\n",
    "    image=torch.from_numpy(image)\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!Make sure normalization goes match above!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    image=T.Normalize(mean=mean, std=std)(image)\n",
    " \n",
    "    image.unsqueeze_(0)\n",
    "    image = image.to(device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "      mask=model(image)['out']\n",
    "      mask=nn.Sigmoid()(mask)\n",
    "      # mask=mask.cpu().detach().numpy()\n",
    "#!!!!!!!!!!!!!!!!!Make sure there are the same number of mask outputs as you trained on!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    # combined_image = image_rescaled = rescale(image, scale, anti_aliasing=True)\n",
    "    for i, mat in enumerate(materials):\n",
    "      mat_mask = mask.cpu().detach().numpy()[0,i,:,:]\n",
    "      mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
    "      mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
    "\n",
    "\n",
    "      distance = ndi.distance_transform_edt(mat_mask)\n",
    "\n",
    "      local_max_coords = feature.peak_local_max(distance, footprint=np.ones((50, 50)))\n",
    "      local_max_mask = np.zeros(distance.shape, dtype=bool)\n",
    "      local_max_mask[tuple(local_max_coords.T)] = True\n",
    "      markers = measure.label(local_max_mask)\n",
    "      \n",
    "      seg1 = segmentation.watershed(-distance, markers, mask=mat_mask)\n",
    "    \n",
    "      # sobel = scipy.ndimage.sobel(mat_mask)\n",
    "\n",
    "      # sobel= ndi.binary_fill_holes(sobel)\n",
    "      # selem=disk(10)\n",
    "      # sobel=binary_erosion(sobel, selem=selem)\n",
    "      \n",
    "      # # # io.imshow(sobel)\n",
    "      # markers = np.zeros_like(sobel)\n",
    "      # foreground, background = 1, 2\n",
    "      # markers[sobel <= 0] = background\n",
    "      # markers[sobel >=1] = foreground\n",
    "      # io.imshow(sobel)\n",
    "\n",
    "      # ws = watershed(sobel, markers,mask=mat_mask, compactness=0.1)\n",
    "      \n",
    "      # seg1 = measure.label(ws == foreground)\n",
    "      \n",
    "\n",
    "      combined_image = np.add(image1, mat_mask, casting=\"unsafe\")\n",
    "      # img3=label2rgb(combined_image, image1, alpha=0.2) \n",
    "      img3=label2rgb(seg1, image1, alpha=0.5, bg_label=0)  \n",
    "      io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
    "      \n",
    "\n",
    "    io.imsave(new_dir_name+'/'+filename.split(following)[0]+'.png', img3)    \n",
    "    #io.imsave(new_dir_name+'/'+filename.split(following)[0]+'.png', seg1)    \n",
    "\n",
    "    # name = file_name.append([filename])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FZUoBXEq-8sa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZUoBXEq-8sa",
    "outputId": "ddffdb29-b0b6-4584-dc1e-b1ef8025e6ca"
   },
   "outputs": [],
   "source": [
    "#makes new directory called \"(directory name here) + name in red\" that your new images go into\n",
    "new_dir_name = seg_output_directory\n",
    "if not os.path.exists(new_dir_name):\n",
    "  os.makedirs(new_dir_name)\n",
    "\n",
    "for mat in materials:\n",
    "  new_dir_name_mat= new_dir_name + \"/\" + mat.name\n",
    "  if not os.path.exists(new_dir_name_mat):\n",
    "    os.makedirs(new_dir_name_mat)\n",
    "filename = filenames[i]\n",
    "\n",
    "image = Image.open(dir_name +'/'+ filenames[i])\n",
    "image1=(image)\n",
    "image1=np.array(image1)\n",
    "image1=rgb2grey(image1)\n",
    "image1 = rescale(image1, scale, anti_aliasing=True)\n",
    "\n",
    "w, h = image.size\n",
    "# print(image.size)\n",
    "#!!!!!!!!!!!!!!!!!!!!Make sure scale matches!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "scale=scale\n",
    "newW, newH = int(scale * w), int(scale * h)\n",
    "image=image.resize((newW, newH))\n",
    "image1 = image1[0:newH, 0:newW] # Add this to resize image1\n",
    "image=np.array(image, dtype=float)\n",
    "new_im=np.zeros((3, newH, newW))\n",
    "new_im[0,:,:]=image[:,:,0]\n",
    "new_im[1,:,:]=image[:,:,1]\n",
    "new_im[2,:,:]=image[:,:,2]\n",
    "image=new_im\n",
    "\n",
    "\n",
    "image=torch.from_numpy(image)\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!Make sure normalization goes match above!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "image=T.Normalize(mean=mean, std=std)(image)\n",
    "\n",
    "image.unsqueeze_(0)\n",
    "image = image.to(device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  mask=model(image)['out']\n",
    "  mask=nn.Sigmoid()(mask)\n",
    "  # mask=mask.cpu().detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S-Ee5Dg081te",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-Ee5Dg081te",
    "outputId": "101efc6a-ffc9-4e00-84ee-f7b10ae42d53"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!!!!!!!!!!!!!!!!!Make sure there are the same number of mask outputs as you trained on!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Iterate over materials to print\n",
    "for mat_name in materials_toprint:\n",
    "  j = [j for j, x in enumerate(materials) if x.name == mat_name][0]\n",
    "  mat = materials[j]\n",
    "\n",
    "  mat_mask = mask.cpu().detach().numpy()[0,j,:,:]\n",
    "  mat_mask[mat_mask >= mat.confidence_threshold] = 255\n",
    "  mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
    "  # Invert the mask\n",
    "  mat_mask = np.invert(np.asarray(mat_mask).astype(\"uint8\"))\n",
    "\n",
    "  distance = ndi.distance_transform_edt(mat_mask)\n",
    "\n",
    "  local_max_coords = feature.peak_local_max(distance, footprint=np.ones((25, 25)))\n",
    "  local_max_mask = np.zeros(distance.shape, dtype=bool)\n",
    "  local_max_mask[tuple(local_max_coords.T)] = True\n",
    "  markers = measure.label(local_max_mask)\n",
    "  \n",
    "  seg1 = segmentation.watershed(-distance, markers, mask=mat_mask)\n",
    "\n",
    "  # sobel = scipy.ndimage.sobel(mat_mask)\n",
    "\n",
    "  # sobel= ndi.binary_fill_holes(sobel)\n",
    "  # selem=disk(10)\n",
    "  # sobel=binary_erosion(sobel, selem=selem)\n",
    "  \n",
    "  # # # io.imshow(sobel)\n",
    "  # markers = np.zeros_like(sobel)\n",
    "  # foreground, background = 1, 2\n",
    "  # markers[sobel <= 0] = background\n",
    "  # markers[sobel >=1] = foreground\n",
    "  # io.imshow(sobel)\n",
    "\n",
    "  # ws = watershed(sobel, markers,mask=mat_mask, compactness=0.1)\n",
    "  \n",
    "  # seg1 = measure.label(ws == foreground)\n",
    "  \n",
    "\n",
    "  combined_image = np.add(image1, mat_mask, casting=\"unsafe\")\n",
    "  # img3=label2rgb(combined_image, image1, alpha=0.2) \n",
    "  img3=label2rgb(seg1, image1, alpha=0.5, bg_label=0)  \n",
    "  io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0]+'_' + mat.name + \"_mask.png\", mat_mask)\n",
    "  \n",
    "\n",
    "io.imsave(new_dir_name+'/'+filename.split(following)[0]+'.png', img3)    \n",
    "#io.imsave(new_dir_name+'/'+filename.split(following)[0]+'.png', seg1)    \n",
    "\n",
    "name = file_name.append([filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rh3KD7qz_bAq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rh3KD7qz_bAq",
    "outputId": "e2d2de4e-f150-4903-c0ed-fe16651db240"
   },
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1iCW7KDe_eUt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iCW7KDe_eUt",
    "outputId": "83a37298-0f2f-43bf-e1cf-4f8868f12ee0"
   },
   "outputs": [],
   "source": [
    "mat.confidence_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XU8uTV1aBSYO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU8uTV1aBSYO",
    "outputId": "3edf1dea-fdca-43a2-9dfb-c13d2d0aa56d"
   },
   "outputs": [],
   "source": [
    "type(mat_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ot7LSshGDp39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "ot7LSshGDp39",
    "outputId": "d9d5b7ee-2719-4d3d-85e5-b1f2e93a2873"
   },
   "outputs": [],
   "source": [
    "np.invert(np.asarray(mat_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E6UruF0RDwn_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6UruF0RDwn_",
    "outputId": "c682e84e-b185-451d-9209-b0b2d628f63f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5lPkOB_8D0AR",
   "metadata": {
    "id": "5lPkOB_8D0AR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TrainValidate_Development_FCNSegmentationModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "virtenv_cuda113",
   "language": "python",
   "name": "virtenv_cuda113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e10c23628f34108b2c07936128358c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c689f1e14da4710a0f000406718f7bf",
      "placeholder": "",
      "style": "IPY_MODEL_4a334a60da00453eba099fa645e31bec",
      "value": " 171M/171M [00:01&lt;00:00, 139MB/s]"
     }
    },
    "282f7540606c4689b6e37fe84f6dc3a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_785bf005029d4fd3bf7828b8abfd97f2",
      "placeholder": "",
      "style": "IPY_MODEL_96bfcf21175e47ecb5f95d020d561dd6",
      "value": "100%"
     }
    },
    "3c689f1e14da4710a0f000406718f7bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a334a60da00453eba099fa645e31bec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64fb06d099564c6cac73014f0f9413ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "785bf005029d4fd3bf7828b8abfd97f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96bfcf21175e47ecb5f95d020d561dd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9773ef196c7249009882912e3969490b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64fb06d099564c6cac73014f0f9413ed",
      "max": 178793939,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cda3514e741b41b0b15a1601bc7f3105",
      "value": 178793939
     }
    },
    "cda3514e741b41b0b15a1601bc7f3105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce3d25a594a340d0b01caf26ad9cbd3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_282f7540606c4689b6e37fe84f6dc3a5",
       "IPY_MODEL_9773ef196c7249009882912e3969490b",
       "IPY_MODEL_1e10c23628f34108b2c07936128358c0"
      ],
      "layout": "IPY_MODEL_e0478ba3833444ed9e865f91abfd9ee0"
     }
    },
    "e0478ba3833444ed9e865f91abfd9ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
