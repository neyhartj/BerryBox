{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d456c6a",
   "metadata": {
    "id": "9d456c6a"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neyhartj/BerryBox/blob/master/deploy_BerryBox_FCNSegmentationModel_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1RJFn5hLaSrw",
   "metadata": {
    "id": "1RJFn5hLaSrw"
   },
   "source": [
    "# BerryBox Image Analysis Pipeline\n",
    "\n",
    "Use this notebook for production applications of a trained fully convolutional network (FCN) to measure quality parameters on individual berries in images.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Prior to running the pipeline, make sure you have completed these setup steps:\n",
    "\n",
    "1. Clone the [`BerryBox` GitHub repository](https://github.com/neyhartj/BerryBox) and place it on Google Drive.\n",
    "2. Place a copy of the trained model in BerryBox/fcn_model_training/productionModel. A trained model is freely available [here]().\n",
    "3. Add images to be fed into the pipeline to BerryBox/imagesToSegment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f_7FuYyMej4V",
   "metadata": {
    "id": "f_7FuYyMej4V"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive - this will ask you to authorize.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "drive = \"/content/drive/MyDrive\"\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "# Install plant cv\n",
    "!pip install plantcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d076506",
   "metadata": {
    "id": "4d076506"
   },
   "source": [
    "\n",
    "# **Pipeline Settings**\n",
    "\n",
    "Edit settings below to run the production pipeline\n",
    "\n",
    "## Materials\n",
    "\n",
    "The **materials** object will be used to store information about the features in the images. For now, the only materials are berries and non-berries (i.e. background). For each material, add the following (in order):\n",
    "\n",
    "\n",
    "\n",
    "> **name** (str) - The name for the material. This is pretty arbitrary, but it will be\n",
    "  used to label output folders and images.  \n",
    "  **input_rbg_vals** (list) - The rbg values of the material in the input mask image.  \n",
    "  **output_val** (int) - The greyscale value of the mask when you output the images.\n",
    "  This is arbitrary, but every material should have its own output color\n",
    "  so they can be differentiated.  \n",
    "  **confidence_threshold** (float) - The lower this number, the more voxels will be labled a specific material. Essentially, the ML algorith outptus a confdience value  (centered on 0.5) for every voxel and every material. By default, voxels with  a confidence of 0.5 or greater are determined to be the material in question.  But we can labled voxles with a lower condience level by changing this  parameter\n",
    "\n",
    "## Other settings\n",
    "\n",
    "**proj_dir**: The path to the project directory (i.e. \"BerryBox\").\n",
    "\n",
    "**inference_dir**: Folder with images to run through the pipeline. Defaults to \"imagesToSegment\".\n",
    "\n",
    "**model_path**: The path to the trained FCN model. It should be located in the BerryBox/fcn_model_training/productionModel directory.\n",
    "\n",
    "**normalization_path**: The path to the normalization data file that was saved during model training.\n",
    "\n",
    "**region_properties**: A list of region properties to extract for the berries. See the [regionprops documentation](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) for details.\n",
    "\n",
    "**max_area**: Maximum area of object (in pixels) to keep that object.\n",
    "\n",
    "**min_area**: Minimum area of object (in pixels) to keep that object.\n",
    "\n",
    "**save_segmentation_image** (boolean): should the pipeline save segmented images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "-a6Oocjd2Wu-",
   "metadata": {
    "id": "-a6Oocjd2Wu-"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Set parameters #########\n",
    "#############################\n",
    "\n",
    "class Material:\n",
    " \n",
    "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
    "    self.name = name\n",
    "    self.input_rgb_vals = input_rgb_vals\n",
    "    self.output_val = output_val\n",
    "    self.confidence_threshold = confidence_threshold\n",
    "\n",
    "# Create a list of materials so we can iterate through it\n",
    "materials = [\n",
    "             Material(\"background\", [0,0,0], 0, 0.5),\n",
    "             Material(\"berry\", [255,255,255], 255, 0.6),\n",
    "             ]\n",
    "\n",
    "\n",
    "# What material would you like to make inferences for?\n",
    "materials_toprint = [\"berry\"]\n",
    "\n",
    "# Project directory\n",
    "# IMPORTANT - ALL DIRECTORIES NEED TO END IN A /\n",
    "proj_dir = drive + \"/project/PATH/TO/BerryBox/\"\n",
    "\n",
    "# Directory of images to segment\n",
    "inference_dir = proj_dir + \"/imagesToSegment\"\n",
    "\n",
    "\n",
    "# Path to the trained FCN model\n",
    "# This file should end in \".pth\"\n",
    "# model_path = proj_dir + \"productionModel/berryBox_fcn_production_v1_model3.pth\"\n",
    "model_path = proj_dir + \"/PATH/TO/MODEL\"\n",
    "# Normalization data path\n",
    "normalization_path = proj_dir + \"/PATH/TO/NORMALIZATION/INFORMATION/\"\n",
    "\n",
    "# Path to the blank image with the color card - this is used for color correction\n",
    "cc_img_path = proj_dir + \"/resources/color_checker_standard1.JPG\"\n",
    "\n",
    "# Properties for regionprops\n",
    "# region_properties = [\"area\", \"axis_major_length\", \"axis_minor_length\", \"eccentricity\"] # For local runs\n",
    "region_properties = [\"area\", \"major_axis_length\", \"minor_axis_length\", \"eccentricity\"] # For colab runs\n",
    "\n",
    "# Maximum object area (in pixels) to keep\n",
    "max_area = 15000\n",
    "# Minimum object area (in pixels) to keep\n",
    "min_area = 200\n",
    "\n",
    "# Should the pipeline save segmented images\n",
    "save_segmentation_image = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X49s7N2WXHKM",
   "metadata": {
    "id": "X49s7N2WXHKM"
   },
   "source": [
    "# **Image Segmentation**\n",
    "\n",
    "Run the image inference pipeline. This pipeline will:\n",
    "1. Read in an inference image, correct color, and identify the QR code.\n",
    "2. Run the image through the prediction model\n",
    "3. Segment the relevant mask\n",
    "4. Identify objects in the image\n",
    "5. Measure object properties and save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-MuczokV7w-A",
   "metadata": {
    "id": "-MuczokV7w-A"
   },
   "source": [
    "## Import packages and define custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nPEGUMpy7zp_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPEGUMpy7zp_",
    "outputId": "f1f71a86-f8d8-4226-b7cd-5a05ef417a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages and functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load relevant packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from plantcv import plantcv as pcv\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation import fcn_resnet101\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from scipy import ndimage as ndi\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.color import rgb2gray, label2rgb\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage.morphology import binary_erosion\n",
    "from skimage.measure import label, regionprops_table, regionprops\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Specify directories\n",
    "# Directory to store output segmented images\n",
    "output_directory = proj_dir + \"/output/\"\n",
    "# Directory of segmented images\n",
    "seg_output_directory = output_directory + \"/segmented_images/\"\n",
    "\n",
    "# Empty and create these directories\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "if not os.path.exists(seg_output_directory):\n",
    "    os.mkdir(seg_output_directory)\n",
    "\n",
    "# How many materials?\n",
    "num_materials = len(materials)\n",
    "\n",
    "# Load a QR code detector\n",
    "detector = cv.QRCodeDetector()\n",
    "\n",
    "# A function to find a QR code within an image\n",
    "# img = an image of type uint8\n",
    "# search_coord = a list of tuples; the first element is a range of scaled heights [0, 1] to search\n",
    "# and the second element is a range of scaled widths [0, 1] to search. \n",
    "def find_qr(img, search_coord = [(0.5, 1), (0.5, 1)]):\n",
    "    # Image dimensions\n",
    "    img_dim = image.shape\n",
    "\n",
    "    # Resize the image\n",
    "    h1 = int(search_coord[0][0] * img_dim[0])\n",
    "    h2 = int(search_coord[0][1] * img_dim[0])\n",
    "    hdiff = h2 - h1\n",
    "    w1 = int(search_coord[1][0] * img_dim[1])\n",
    "    w2 = int(search_coord[1][1] * img_dim[1])\n",
    "    wdiff = w2 - w1\n",
    "\n",
    "    img_resize = img[h1:h2, w1:w2, :]\n",
    "\n",
    "    # Iterate until a QR code is found\n",
    "    collection_id = \"\"\n",
    "    iterator = 0\n",
    "    # Grid size, rows x cols\n",
    "    grid_size_list = [(1, 1), (3, 1), (2, 2), (3, 2), (2, 3), (3, 3)]\n",
    "\n",
    "    for split_grid_size in grid_size_list: \n",
    "        # Get the dimensions of each element of the grid\n",
    "        grid_h = int(hdiff / split_grid_size[0])\n",
    "        grid_w = int(wdiff / split_grid_size[1])\n",
    "        # Starting points\n",
    "        start_rows = [x for x in range(0, hdiff, grid_h)]\n",
    "        start_cols = [x for x in range(0, wdiff, grid_w)]\n",
    "        start_rows.reverse()\n",
    "        start_cols.reverse()\n",
    "\n",
    "        for j, r0 in enumerate(start_rows):\n",
    "            for i, c0 in enumerate(start_cols):\n",
    "                r1 = r0 + grid_h\n",
    "                c1 = c0 + grid_w\n",
    "\n",
    "                # Crop the image\n",
    "                img2_crop = img_resize[r0:r1, c0:c1]\n",
    "\n",
    "                # Attempt to find the QR code\n",
    "                collection_id, points, _ = detector.detectAndDecode(img2_crop)\n",
    "\n",
    "                # If the QR code is found, stop\n",
    "                if collection_id != \"\":\n",
    "                    break\n",
    "\n",
    "            # Again break\n",
    "            if collection_id != \"\":\n",
    "                break\n",
    "\n",
    "        # Again break\n",
    "        if collection_id != \"\":\n",
    "            break\n",
    "\n",
    "    # Return the cropped image and the collection id\n",
    "    return img_resize, collection_id\n",
    "\n",
    "\n",
    "print(\"Packages and functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcBn0NewLaY9",
   "metadata": {
    "id": "lcBn0NewLaY9"
   },
   "source": [
    "## Load the trained model, normalization metadata, and color card correction standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cHQUl0rWLfqP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHQUl0rWLfqP",
    "outputId": "fadd797c-a4e8-48a6-b81a-d086af7d4f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Name of the model group\n",
    "current_model_name = os.path.basename(model_path).replace(\".pth\", \"\")\n",
    "model_group = current_model_name.split(\"_model\")[0]\n",
    "\n",
    "# Load a pretrained model\n",
    "model = fcn_resnet101(pretrained=False)\n",
    "model.classifier=FCNHead(2048, num_materials)\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    " \n",
    "# Load the model specified above\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "model.train()\n",
    "\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "\n",
    "## Load the normalization information ##\n",
    "# Read in the important model training log information\n",
    "with open(normalization_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        # Strip off newline; separate by tab\n",
    "        tabs = line.strip().split(\"\\t\")\n",
    "\n",
    "        # Get the name of the variable; this will be used for assignment\n",
    "        var_name = tabs[0]\n",
    "\n",
    "        # Parse the second tab\n",
    "        if tabs[1].startswith(\"tensor\"):\n",
    "\n",
    "            # Create a vector of numeric characters\n",
    "            var_value = tabs[1].split(\"[\")[1].split(\"]\")[0].split(\", \")\n",
    "            # Convert this to numeric\n",
    "            var_value = [float(x) for x in var_value]\n",
    "            # Convert to np array; then to tensor\n",
    "            var_value = np.array(var_value)\n",
    "            var_value = torch.tensor(var_value)\n",
    "\n",
    "        else:\n",
    "            var_value = float(tabs[1])\n",
    "\n",
    "        # Assign variable name\n",
    "        vars()[var_name] = var_value\n",
    "        \n",
    "# assign to mean and std\n",
    "mean = normalization_mean\n",
    "std = normalization_std\n",
    "newW = int(image_scale_newW)\n",
    "newH = int(image_scale_newH)\n",
    "\n",
    "# Find the color card in the source file\n",
    "# Read in the color checker standard file\n",
    "cc_img = np.array(Image.open(cc_img_path).resize((newW, newH)), dtype = \"uint8\")\n",
    "# Find the color card in the color checker standard file\n",
    "df1, start, space = pcv.transform.find_color_card(rgb_img = cc_img)\n",
    "# Create a mask\n",
    "# Use these outputs to create a labeled color card mask\n",
    "target_mask = pcv.transform.create_color_card_mask(rgb_img = cc_img, radius = 25, start_coord = start, \n",
    "                                                   spacing = space, ncols = 4, nrows = 6)\n",
    "# get color matrix of target and save\n",
    "target_headers, target_matrix = pcv.transform.get_color_matrix(cc_img, target_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7qVnO9B8IK-",
   "metadata": {
    "id": "S7qVnO9B8IK-"
   },
   "source": [
    "## Run the image processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TeHYoalnemaR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeHYoalnemaR",
    "outputId": "1eca376b-79c0-4219-f711-41abd4b3f201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861 images found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:41,  5.02s/it]"
     ]
    }
   ],
   "source": [
    "## Iterate over images and run through the prediction model ##\n",
    "\n",
    "# Rename the directory containing the images to segment\n",
    "dir_name = inference_dir\n",
    "filenames = [x for x in os.listdir(dir_name) if (\"JPG\" in x.upper() or \"PNG\" in x.upper())]\n",
    "filenames.sort()\n",
    "print(str(len(filenames)) + \" images found.\")\n",
    "\n",
    "# Create an empty list to store region data\n",
    "all_region_df = []\n",
    "\n",
    "# Iterate over the images\n",
    "# tqdm produces a progress bar\n",
    "for i, filename in tqdm(enumerate(filenames)):\n",
    "\n",
    "# # ## TESTING ##\n",
    "# i = 0\n",
    "# filename = filenames[i]\n",
    "# ################\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(dir_name +'/'+ filename)\n",
    "\n",
    "    # Rescale the image\n",
    "    image = image.resize((newW, newH))\n",
    "    # Convert image to NP array\n",
    "    # Need to use uint8 for the QR code detector and the color correction\n",
    "    image = np.array(image, dtype = \"uint8\")\n",
    "\n",
    "    ###\n",
    "    # Find and read the QR code\n",
    "    ###\n",
    "    # the function returns the cropped image and the collection id\n",
    "    img1_resize, cid = find_qr(image)\n",
    "\n",
    "    ###\n",
    "    # Pixel size determination\n",
    "    ###\n",
    "\n",
    "    # Try to find the color card\n",
    "    try:\n",
    "        df1, start, space = pcv.transform.find_color_card(rgb_img = img1_resize)\n",
    "        color_card_found = True\n",
    "    except:\n",
    "        color_card_found = False\n",
    "\n",
    "    # IF the color card was not found, skip pixel scaling and color correction\n",
    "    if color_card_found:\n",
    "\n",
    "        # Calculate the average box width and height\n",
    "        box_w = np.mean(df1['width'])\n",
    "        box_h = np.mean(df1['height'])\n",
    "        # We know boxes are about 1.1581 cm on each side (square this to get area)\n",
    "        # Calculate the number of pixel per cm\n",
    "        pixel_per_cm = np.mean([x / 1.1581 for x in [box_w, box_h]])\n",
    "        cm_per_pixel = 1 / pixel_per_cm\n",
    "        # Recalculate the pixels per cm\n",
    "        cm2_per_pixel = cm_per_pixel ** 2\n",
    "\n",
    "        ###\n",
    "        # Color correction\n",
    "        ###\n",
    "\n",
    "        # Create a mask\n",
    "        # Use these outputs to create a labeled color card mask\n",
    "        # The radius setting needs to be large enough to capture the color on each square, but not\n",
    "        # too big as to overlap with adjacent squares.\n",
    "        source_mask = pcv.transform.create_color_card_mask(rgb_img = img1_resize, radius = 15, start_coord = start, \n",
    "                                                            spacing = space, ncols = 4, nrows = 6)\n",
    "        # Get the source matrix\n",
    "        source_headers, source_matrix = pcv.transform.get_color_matrix(img1_resize, source_mask)\n",
    "        ## Run color correction ##\n",
    "        # matrix_a is a matrix of average rgb values for each color ship in source_img, matrix_m is a moore-penrose inverse matrix,\n",
    "        # matrix_b is a matrix of average rgb values for each color ship in source_img\n",
    "        matrix_a, matrix_m, matrix_b = pcv.transform.get_matrix_m(target_matrix = target_matrix, source_matrix = source_matrix)\n",
    "        # deviance is the measure of how greatly the source image deviates from the target image's color space. \n",
    "        # Two images of the same color space should have a deviance of ~0.\n",
    "        # transformation_matrix is a 9x9 matrix of transformation coefficients \n",
    "        deviance, transformation_matrix = pcv.transform.calc_transformation_matrix(matrix_m, matrix_b)\n",
    "\n",
    "        image = pcv.transform.apply_transformation_matrix(source_img = image, target_img = cc_img, transformation_matrix = transformation_matrix)\n",
    "\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    ###\n",
    "    # Use the FCN model to predict berry pixels\n",
    "    ###\n",
    "\n",
    "    # Create a tensor from the image\n",
    "    image = image.astype(\"float\")\n",
    "    image_tensor = np.zeros((3, newH, newW))\n",
    "    image_tensor[0,:,:] = image[:,:,0]\n",
    "    image_tensor[1,:,:] = image[:,:,1]\n",
    "    image_tensor[2,:,:] = image[:,:,2]\n",
    "    image_tensor = torch.from_numpy(image_tensor)\n",
    "    # Normalize the tensor and send it to the GPU\n",
    "    image_tensor = T.Normalize(mean=mean, std=std)(image_tensor)\n",
    "    image_tensor.unsqueeze_(0)\n",
    "    image_tensor = image_tensor.to(device=device, dtype=torch.float32)\n",
    "    # Convert image to uint8 for downstream\n",
    "    image = image.astype(\"uint8\")\n",
    "\n",
    "    # Run the image through the prediction model\n",
    "    with torch.no_grad():\n",
    "        mask = model(image_tensor)['out']\n",
    "        mask = nn.Sigmoid()(mask)\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "\n",
    "    ###\n",
    "    # Measure berry properties\n",
    "    ###\n",
    "\n",
    "    # Iterate over materials to print\n",
    "    for mat_to_print in materials_toprint:\n",
    "\n",
    "        # Find the index of this material in the materials list\n",
    "        mat_idx = [i for i, x in enumerate(materials) if x.name == mat_to_print][0]\n",
    "        # Get the material at this index\n",
    "        mat = materials[mat_idx]\n",
    "        # Get the mask from the prediction model at this index\n",
    "        mat_mask = mask[0,mat_idx,:,:]\n",
    "        mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
    "        mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
    "\n",
    "        # Perform object segmentation and regionprop calculation\n",
    "        # This is from https://github.com/danforthcenter/plantcv/blob/master/plantcv/plantcv/watershed.py\n",
    "        # Convert the mat_mask to 8-bit\n",
    "        mat_mask = mat_mask.astype(\"uint8\")\n",
    "\n",
    "        # Run watershed here? Or binary erosion?\n",
    "        # For now, skip\n",
    "        # Run distance transform\n",
    "        # dist_transform = cv.distanceTransformWithLabels(mat_mask, distanceType = cv.DIST_L2, maskSize = 0)[0]\n",
    "        # local_max = feature.peak_local_max(dist_transform, indices = False, min_distance = distance, labels = mat_mask)\n",
    "        # markers = ndi.label(local_max, structure=np.ones((3, 3)))[0]\n",
    "        # dist_transform1 = -dist_transform\n",
    "        # mat_mask = segmentation.watershed(dist_transform1, markers, mask = mat_mask)\n",
    "\n",
    "        ## Estimate berry traits\n",
    "        # Label the segmentation output\n",
    "        label_mat = label(np.array(mat_mask), background = 0)\n",
    "\n",
    "        # Regionprops\n",
    "        region_properties1 = list(set([\"label\", \"bbox\"] + region_properties))\n",
    "        region_properties_names = region_properties1 + [x + \"_intensity_mean\" for x in [\"red\", \"green\", \"blue\"]] + [x + \"_intensity_sd\" for x in [\"red\", \"green\", \"blue\"]]\n",
    "        region_properties_names = region_properties_names + [x + \"_intensity_mean\" for x in [\"hue\", \"sat\", \"val\"]] + [x + \"_intensity_sd\" for x in [\"hue\", \"sat\", \"val\"]]\n",
    "        region_properties_names = tuple([\"file_name\", \"collection_id\", \"color_corrected\", \"material\", \"label\"] + [x for x in region_properties_names if x != \"label\"])\n",
    "\n",
    "        # Empty dictionary to store data\n",
    "        regions_dict = {}\n",
    "\n",
    "        # Initialize lists in the dictionary\n",
    "        for key in region_properties_names:\n",
    "            regions_dict[key] = []\n",
    "\n",
    "        # Iterate over regions in the image\n",
    "        for region in regionprops(label_image = label_mat):\n",
    "\n",
    "            # Add manual keys\n",
    "            regions_dict[\"file_name\"] = filename\n",
    "            regions_dict[\"collection_id\"] = cid\n",
    "            regions_dict[\"color_corrected\"] = str(color_card_found)\n",
    "            regions_dict[\"material\"] = mat_to_print\n",
    "\n",
    "            # Add props to the dictionary\n",
    "            for prop in region_properties1:\n",
    "                regions_dict[prop].append(region[prop])\n",
    "\n",
    "\n",
    "            # Convert image to HSV\n",
    "            image_hsv = cv.cvtColor(image, cv.COLOR_RGB2HSV)\n",
    "\n",
    "            berry_rgb_values = []\n",
    "            berry_hsv_values = []\n",
    "\n",
    "            for y, x in region.coords:\n",
    "                berry_rgb_values.append(image[y, x, :])\n",
    "                berry_hsv_values.append(image_hsv[y, x, :])\n",
    "\n",
    "            for c, left in enumerate([\"red\", \"green\", \"blue\"]):\n",
    "                key = left + \"_intensity_mean\"\n",
    "                vals = [x[c] for x in berry_rgb_values]\n",
    "                regions_dict[key].append(np.mean(vals))\n",
    "\n",
    "                key = key.replace(\"mean\", \"sd\")\n",
    "                regions_dict[key].append(np.std(vals))\n",
    "\n",
    "            for c, left in enumerate([\"hue\", \"sat\", \"val\"]):\n",
    "                key = left + \"_intensity_mean\"\n",
    "                vals = [x[c] for x in berry_hsv_values]\n",
    "                regions_dict[key].append(np.mean(vals))\n",
    "\n",
    "                key = key.replace(\"mean\", \"sd\")\n",
    "                regions_dict[key].append(np.std(vals))\n",
    "\n",
    "        # Convert the regions_dict to a data.frame\n",
    "        regions_df = pd.DataFrame(regions_dict)\n",
    "\n",
    "        # Remove excessively large regions\n",
    "        regions_df = regions_df[(regions_df[\"area\"] <= max_area) & (regions_df[\"area\"] >= min_area)]\n",
    "\n",
    "        # Convert area, length, width to cm\n",
    "        regions_df[\"area\"] = regions_df[\"area\"] * cm2_per_pixel\n",
    "        regions_df[\"major_axis_length\"] = regions_df[\"major_axis_length\"] * cm_per_pixel\n",
    "        regions_df[\"minor_axis_length\"] = regions_df[\"minor_axis_length\"] * cm_per_pixel\n",
    "\n",
    "        # Save the region data\n",
    "        all_region_df.append(regions_df)\n",
    "\n",
    "        ### Save a segmentation image ###\n",
    "        if save_segmentation_image:\n",
    "            image = label2rgb(label_mat, image, alpha=0.3, bg_label = 0)\n",
    "            regions_df_use = regions_df.to_dict()\n",
    "\n",
    "            # Iterate over the berry index\n",
    "            for lab in regions_df_use[\"label\"]:\n",
    "                bbox = regions_df_use[\"bbox\"][lab]\n",
    "                # draw rectangle around segmented coins\n",
    "                minr, minc, maxr, maxc = bbox\n",
    "                cv.rectangle(image, (minc, minr), (maxc, maxr),(0,255,0),2)\n",
    "\n",
    "            image = Image.fromarray((image * 255).astype(\"uint8\"))\n",
    "            # New filename\n",
    "            ext = filename.split(\".\")[-1]\n",
    "            new_filename = filename.replace(ext, \"PNG\")\n",
    "            image.save(seg_output_directory + \"/\" + mat_to_print + \"-segmented-\" + cid + \"-\" + new_filename)\n",
    "\n",
    "# Save the region data\n",
    "# Merge the region data.feames\n",
    "all_regions_data = pd.concat(all_region_df)\n",
    "# Drop the bbox column\n",
    "all_regions_data = all_regions_data.drop(columns = \"bbox\")\n",
    "region_filename = output_directory + \"/\" + current_model_name + \"_InferenceImageRegionData.csv\"\n",
    "all_regions_data.to_csv(region_filename, index = False)\n",
    "            \n",
    "print(\"\\nImage analysis pipeline complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-MuczokV7w-A",
    "lcBn0NewLaY9"
   ],
   "name": "deploy_BerryBox_FCNSegmentationModel_Colab_2021ImageAnalysis.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
