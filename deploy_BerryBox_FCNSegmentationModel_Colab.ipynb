{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d456c6a",
      "metadata": {
        "id": "9d456c6a"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neyhartj/BerryBox/blob/master/deploy_BerryBox_FCNSegmentationModel_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1RJFn5hLaSrw",
      "metadata": {
        "id": "1RJFn5hLaSrw"
      },
      "source": [
        "# BerryBox Image Analysis Pipeline\n",
        "\n",
        "Use this notebook for production applications of a trained fully convolutional network (FCN) to measure quality parameters on individual berries in images.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Prior to running the pipeline, make sure you have completed these setup steps:\n",
        "\n",
        "1. Clone the [`BerryBox` GitHub repository](https://github.com/neyhartj/BerryBox) and place it on Google Drive.\n",
        "2. Place a copy of the trained model in BerryBox/fcn_model_training/productionModel. A trained model is freely available [here]().\n",
        "3. Add images to be fed into the pipeline to BerryBox/imagesToSegment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f_7FuYyMej4V",
      "metadata": {
        "id": "f_7FuYyMej4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8909a3c-6889-47ad-d78e-bd7ed11d25d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Wed Jul  6 13:45:04 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plantcv\n",
            "  Downloading plantcv-3.14-py3-none-any.whl (285 kB)\n",
            "\u001b[K     |████████████████████████████████| 285 kB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from plantcv) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from plantcv) (2.8.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from plantcv) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.4.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from plantcv) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.21.6)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.10.2)\n",
            "Requirement already satisfied: scikit-image>=0.13 in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.18.3)\n",
            "Collecting dask-jobqueue\n",
            "  Downloading dask_jobqueue-0.7.3-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (1.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5->plantcv) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->plantcv) (1.15.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (7.1.2)\n",
            "Collecting dask\n",
            "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.8 MB/s \n",
            "\u001b[?25hCollecting distributed>=2.19\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 59.2 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (0.11.2)\n",
            "Collecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (21.3)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (1.3.0)\n",
            "Collecting cloudpickle>=1.1.1\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.2.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (5.1.1)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.11.3)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (5.4.8)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.19->dask-jobqueue->plantcv) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.19->dask-jobqueue->plantcv) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->plantcv) (2022.1)\n",
            "Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.6.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.5.2)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (1.1.0)\n",
            "Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine->plantcv) (3.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->plantcv) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->plantcv) (3.1.0)\n",
            "Installing collected packages: locket, pyyaml, partd, fsspec, cloudpickle, dask, distributed, dask-jobqueue, plantcv\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.1.0 dask-2022.2.0 dask-jobqueue-0.7.3 distributed-2022.2.0 fsspec-2022.5.0 locket-1.0.0 partd-1.2.0 plantcv-3.14 pyyaml-6.0\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive - this will ask you to authorize.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "drive = \"/content/drive/MyDrive\"\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "# Install plant cv\n",
        "!pip install plantcv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d076506",
      "metadata": {
        "id": "4d076506"
      },
      "source": [
        "\n",
        "# **Pipeline Settings**\n",
        "\n",
        "Edit settings below to run the production pipeline\n",
        "\n",
        "## Materials\n",
        "\n",
        "The **materials** object will be used to store information about the features in the images. For now, the only materials are berries and non-berries (i.e. background). For each material, add the following (in order):\n",
        "\n",
        "\n",
        "\n",
        "> **name** (str) - The name for the material. This is pretty arbitrary, but it will be\n",
        "  used to label output folders and images.  \n",
        "  **input_rbg_vals** (list) - The rbg values of the material in the input mask image.  \n",
        "  **output_val** (int) - The greyscale value of the mask when you output the images.\n",
        "  This is arbitrary, but every material should have its own output color\n",
        "  so they can be differentiated.  \n",
        "  **confidence_threshold** (float) - The lower this number, the more voxels will be labled a specific material. Essentially, the ML algorith outptus a confdience value  (centered on 0.5) for every voxel and every material. By default, voxels with  a confidence of 0.5 or greater are determined to be the material in question.  But we can labled voxles with a lower condience level by changing this  parameter\n",
        "\n",
        "## Other settings\n",
        "\n",
        "**proj_dir**: The path to the project directory (i.e. \"BerryBox\").\n",
        "\n",
        "**inference_dir**: Folder with images to run through the pipeline. Defaults to \"imagesToSegment\".\n",
        "\n",
        "**model_path**: The path to the trained FCN model. It should be located in the BerryBox/fcn_model_training/productionModel directory.\n",
        "\n",
        "**normalization_path**: The path to the normalization data file that was saved during model training.\n",
        "\n",
        "**region_properties**: A list of region properties to extract for the berries. See the [regionprops documentation](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) for details.\n",
        "\n",
        "**max_area**: Maximum area of object (in pixels) to keep that object.\n",
        "\n",
        "**min_area**: Minimum area of object (in pixels) to keep that object.\n",
        "\n",
        "**save_segmentation_image** (boolean): should the pipeline save segmented images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-a6Oocjd2Wu-",
      "metadata": {
        "id": "-a6Oocjd2Wu-"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### Set parameters #########\n",
        "#############################\n",
        "\n",
        "class Material:\n",
        " \n",
        "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
        "    self.name = name\n",
        "    self.input_rgb_vals = input_rgb_vals\n",
        "    self.output_val = output_val\n",
        "    self.confidence_threshold = confidence_threshold\n",
        "\n",
        "# Create a list of materials so we can iterate through it\n",
        "materials = [\n",
        "             Material(\"background\", [0,0,0], 0, 0.5),\n",
        "             Material(\"berry\", [255,255,255], 255, 0.5),\n",
        "             ]\n",
        "\n",
        "\n",
        "# What material would you like to make inferences for?\n",
        "materials_toprint = [\"berry\"]\n",
        "\n",
        "# Project directory\n",
        "# IMPORTANT - ALL DIRECTORIES NEED TO END IN A /\n",
        "proj_dir = drive + \"/ImageAnalysis/BerryBox/\"\n",
        "\n",
        "# Directory of images to segment\n",
        "inference_dir = proj_dir + \"/imagesToSegment\"\n",
        "\n",
        "\n",
        "# Path to the trained FCN model\n",
        "# This file should end in \".pth\"\n",
        "# model_path = proj_dir + \"productionModel/berryBox_fcn_production_v1_model3.pth\"\n",
        "model_path = proj_dir + \"productionModel/berryBox_fcn_developmental_v4_model3.pth\"\n",
        "\n",
        "\n",
        "# Normalization data path\n",
        "normalization_path = proj_dir + \"productionModel/berryBox_fcn_developmental_v4_model_normalization_param.txt\"\n",
        "\n",
        "# Path to the blank image with the color card - this is used for color correction\n",
        "cc_img_path = proj_dir + \"/resources/color_checker_standard1.JPG\"\n",
        "\n",
        "# Properties for regionprops\n",
        "# region_properties = [\"area\", \"axis_major_length\", \"axis_minor_length\", \"eccentricity\"] # For local runs\n",
        "region_properties = [\"area\", \"major_axis_length\", \"minor_axis_length\", \"eccentricity\"] # For colab runs\n",
        "\n",
        "# Maximum object area (in pixels) to keep\n",
        "max_area = 15000\n",
        "# Minimum object area (in pixels) to keep\n",
        "min_area = 200\n",
        "\n",
        "# Should the pipeline save segmented images\n",
        "save_segmentation_image = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X49s7N2WXHKM",
      "metadata": {
        "id": "X49s7N2WXHKM"
      },
      "source": [
        "# **Image Segmentation**\n",
        "\n",
        "Run the image inference pipeline. This pipeline will:\n",
        "1. Read in an inference image, correct color, and identify the QR code.\n",
        "2. Run the image through the prediction model\n",
        "3. Segment the relevant mask\n",
        "4. Identify objects in the image\n",
        "5. Measure object properties and save the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-MuczokV7w-A",
      "metadata": {
        "id": "-MuczokV7w-A"
      },
      "source": [
        "## Import packages and load a specific model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nPEGUMpy7zp_",
      "metadata": {
        "id": "nPEGUMpy7zp_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "3766c16f77b246db91426c6c42c3d60f",
            "8e2c44b2667f4673839ece1322bb76ee",
            "2af316247e8a46dbbabb97fe418f65d3",
            "06ea2f9876f6405dad8f75919f762c32",
            "b3d14fde36944240a591e09134306add",
            "8e42f673d2214b0783dfa096d182fab9",
            "84603b4865ed4b47aa64c6f3739d82cc",
            "c99fea21ce9a40c8a306213f6c8b8d34",
            "47dce6ff41a74366a62734a5ff83a725",
            "322ecdd18ee748e58c60768b0af5827b",
            "39c01703fdec4c349c538c80cdb69586"
          ]
        },
        "outputId": "13fb2926-56e6-4e76-e38a-2a47cdbb5cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3766c16f77b246db91426c6c42c3d60f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load relevant packages\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from plantcv import plantcv as pcv\n",
        "from torchvision.models.segmentation.fcn import FCNHead\n",
        "from torchvision.models.segmentation import fcn_resnet101\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from scipy import ndimage as ndi\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from skimage.color import rgb2gray, label2rgb\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from skimage.morphology import binary_erosion\n",
        "from skimage.measure import label, regionprops_table, regionprops\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Name of the model group\n",
        "current_model_name = os.path.basename(model_path).replace(\".pth\", \"\")\n",
        "model_group = current_model_name.split(\"_model\")[0]\n",
        "\n",
        "\n",
        "## Specify directories\n",
        "# Directory to store output segmented images\n",
        "output_directory = proj_dir + \"/output/\"\n",
        "# Directory of segmented images\n",
        "seg_output_directory = output_directory + \"/segmented_images/\"\n",
        "\n",
        "# Empty and create these directories\n",
        "if not os.path.exists(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "if not os.path.exists(seg_output_directory):\n",
        "    os.mkdir(seg_output_directory)\n",
        "\n",
        "# How many materials?\n",
        "num_materials = len(materials)\n",
        "\n",
        "# Load a pretrained model\n",
        "model = fcn_resnet101(pretrained=False)\n",
        "model.classifier=FCNHead(2048, num_materials)\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        " \n",
        "# Load the model specified above\n",
        "model.load_state_dict(torch.load(model_path), strict=False)\n",
        "model.train()\n",
        "\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "\n",
        "## Load the normalization information ##\n",
        "# Read in the important model training log information\n",
        "with open(normalization_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Strip off newline; separate by tab\n",
        "        tabs = line.strip().split(\"\\t\")\n",
        "\n",
        "        # Get the name of the variable; this will be used for assignment\n",
        "        var_name = tabs[0]\n",
        "\n",
        "        # Parse the second tab\n",
        "        if tabs[1].startswith(\"tensor\"):\n",
        "\n",
        "            # Create a vector of numeric characters\n",
        "            var_value = tabs[1].split(\"[\")[1].split(\"]\")[0].split(\", \")\n",
        "            # Convert this to numeric\n",
        "            var_value = [float(x) for x in var_value]\n",
        "            # Convert to np array; then to tensor\n",
        "            var_value = np.array(var_value)\n",
        "            var_value = torch.tensor(var_value)\n",
        "\n",
        "        else:\n",
        "            var_value = float(tabs[1])\n",
        "\n",
        "        # Assign variable name\n",
        "        vars()[var_name] = var_value\n",
        "        \n",
        "# assign to mean and std\n",
        "mean = normalization_mean\n",
        "std = normalization_std\n",
        "newW = int(image_scale_newW)\n",
        "newH = int(image_scale_newH)\n",
        "\n",
        "# Find the color card in the source file\n",
        "# Read in the color checker standard file\n",
        "cc_img = np.array(Image.open(cc_img_path).resize((newW, newH)), dtype = \"uint8\")\n",
        "# Find the color card in the color checker standard file\n",
        "df1, start, space = pcv.transform.find_color_card(rgb_img = cc_img)\n",
        "# Create a mask\n",
        "# Use these outputs to create a labeled color card mask\n",
        "target_mask = pcv.transform.create_color_card_mask(rgb_img = cc_img, radius = 25, start_coord = start, \n",
        "                                                   spacing = space, ncols = 4, nrows = 6)\n",
        "# get color matrix of target and save\n",
        "target_headers, target_matrix = pcv.transform.get_color_matrix(cc_img, target_mask)\n",
        "\n",
        "\n",
        "# Load a QR code detector\n",
        "detector = cv.QRCodeDetector()\n",
        "\n",
        "# Create a function to iterate over image grids\n",
        "def find_qr_grids(img, sr, sc, gh, gw):\n",
        "    for j, r0 in enumerate(sr):\n",
        "        for i, c0 in enumerate(sc):\n",
        "            r1 = r0 + gh\n",
        "            c1 = c0 + gw\n",
        "\n",
        "            # Crop the image\n",
        "            img2_crop = img[r0:r1, c0:c1]\n",
        "\n",
        "            # Attempt to find the QR code\n",
        "            collection_id, points, _ = detector.detectAndDecode(img2_crop)\n",
        "\n",
        "            # If the QR code is found, stop\n",
        "            if collection_id != \"\":\n",
        "                # print(\"QR code found for sample: \" + collection_id)\n",
        "                return collection_id, points, i, j\n",
        "            \n",
        "    # Alert user if no QR code was found\n",
        "    if collection_id == \"\":\n",
        "        # print(\"No QR code found.\")\n",
        "        return \"\", \"\", i, j\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S7qVnO9B8IK-",
      "metadata": {
        "id": "S7qVnO9B8IK-"
      },
      "source": [
        "## Run the image processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TeHYoalnemaR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeHYoalnemaR",
        "outputId": "a3b5d619-9557-4dff-bfaf-dca03966638d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 images found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [01:21,  8.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Image analysis pipeline complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "## Iterate over images and run through the prediction model ##\n",
        "\n",
        "# Rename the directory containing the images to segment\n",
        "dir_name = inference_dir\n",
        "filenames = [x for x in os.listdir(dir_name) if (\"JPG\" in x.upper() or \"PNG\" in x.upper())]\n",
        "filenames.sort()\n",
        "print(str(len(filenames)) + \" images found.\")\n",
        "\n",
        "# Create an empty list to store region data\n",
        "all_region_df = []\n",
        "\n",
        "# Iterate over the images\n",
        "# tqdm produces a progress bar\n",
        "for i, filename in tqdm(enumerate(filenames)):\n",
        "\n",
        "# # ## TESTING ##\n",
        "# i = 0\n",
        "# filename = filenames[i]\n",
        "# ################\n",
        "    \n",
        "    # Open the image\n",
        "    image = Image.open(dir_name +'/'+ filename)\n",
        "\n",
        "    # If the image is not PNG, convert to PNG\n",
        "    if not \"PNG\" in filename.upper():\n",
        "        # Get the extension\n",
        "        ext = filename.split(\".\")[-1]\n",
        "        filename = filename.replace(ext, \"PNG\")\n",
        "        image.save(dir_name +'/'+ filename)\n",
        "        # Read the image back in\n",
        "        image = Image.open(dir_name +'/'+ filename)\n",
        "\n",
        "    # Rescale the image\n",
        "    image = image.resize((newW, newH))\n",
        "    # Convert to gray\n",
        "    image_gray = np.asarray(image)\n",
        "    image_gray = rgb2gray(image_gray)\n",
        "    # Convert image to NP array\n",
        "    # Need to use uint8 for the QR code detector and the color correction\n",
        "    image = np.array(image, dtype = \"uint8\")\n",
        "\n",
        "    ###\n",
        "    # Find and read the QR code\n",
        "    ###\n",
        "\n",
        "    # Crop the image to speed up the QR code and color card finder\n",
        "    half_newW = int(newW / 2)\n",
        "    img1_resize = image[:, half_newW:newW, :]\n",
        "\n",
        "    # Split images into a grid\n",
        "    # Grid size, rows x cols\n",
        "    split_grid_size = (3, 1)\n",
        "    # Get the dimensions of each element of the grid\n",
        "    grid_h = int(newH / split_grid_size[0])\n",
        "    grid_w = int(half_newW / split_grid_size[1])\n",
        "    # Starting points\n",
        "    start_rows = [x for x in range(0, newH, grid_h)]\n",
        "    start_cols = [x for x in range(0, half_newW, grid_w)]\n",
        "    start_rows.reverse()\n",
        "    start_cols.reverse()\n",
        "    # Find the QR code\n",
        "    # When a QR code is not found, cid is an empty string\n",
        "    cid, pts, i, j = find_qr_grids(img = img1_resize, sr = start_rows, sc = start_cols, gh = grid_h, gw = grid_w)\n",
        "\n",
        "\n",
        "    ###\n",
        "    # Pixel size determination\n",
        "    ###\n",
        "\n",
        "    # ## Find the color checker card\n",
        "    # # Downsize\n",
        "    # h, w, d = img1.shape\n",
        "    # scale_percent = 50 # percent of original size\n",
        "    # new_h = int(h * scale_percent / 100)\n",
        "    # new_w = int(w * scale_percent / 100)\n",
        "\n",
        "    # # Crop and scale the image\n",
        "    # half_new_w = int(new_w / 2)\n",
        "    # img1_resize = cv.resize(img1, (new_w, new_h))[:, half_new_w:new_w, :]\n",
        "\n",
        "    # Try to find the color card\n",
        "    try:\n",
        "        df1, start, space = pcv.transform.find_color_card(rgb_img = img1_resize)\n",
        "        color_card_found = True\n",
        "    except:\n",
        "        color_card_found = False\n",
        "\n",
        "    # IF the color card was not found, skip pixel scaling and color correction\n",
        "    if color_card_found:\n",
        "\n",
        "        # Calculate the average box width and height\n",
        "        box_w = np.mean(df1['width'])\n",
        "        box_h = np.mean(df1['height'])\n",
        "        # We know boxes are about 1.1581 cm on each side (square this to get area)\n",
        "        # Calculate the number of pixel per cm\n",
        "        pixel_per_cm = np.mean([x / 1.1581 for x in [box_w, box_h]])\n",
        "        cm_per_pixel = 1 / pixel_per_cm\n",
        "        # Recalculate the pixels per cm\n",
        "        cm2_per_pixel = cm_per_pixel ** 2\n",
        "\n",
        "        ###\n",
        "        # Color correction\n",
        "        ###\n",
        "\n",
        "        # Create a mask\n",
        "        # Use these outputs to create a labeled color card mask\n",
        "        # The radius setting needs to be large enough to capture the color on each square, but not\n",
        "        # too big as to overlap with adjacent squares.\n",
        "        source_mask = pcv.transform.create_color_card_mask(rgb_img = img1_resize, radius = 15, start_coord = start, \n",
        "                                                            spacing = space, ncols = 4, nrows = 6)\n",
        "        # Get the source matrix\n",
        "        source_headers, source_matrix = pcv.transform.get_color_matrix(img1_resize, source_mask)\n",
        "        ## Run color correction ##\n",
        "        # matrix_a is a matrix of average rgb values for each color ship in source_img, matrix_m is a moore-penrose inverse matrix,\n",
        "        # matrix_b is a matrix of average rgb values for each color ship in source_img\n",
        "        matrix_a, matrix_m, matrix_b = pcv.transform.get_matrix_m(target_matrix = target_matrix, source_matrix = source_matrix)\n",
        "        # deviance is the measure of how greatly the source image deviates from the target image's color space. \n",
        "        # Two images of the same color space should have a deviance of ~0.\n",
        "        # transformation_matrix is a 9x9 matrix of transformation coefficients \n",
        "        deviance, transformation_matrix = pcv.transform.calc_transformation_matrix(matrix_m, matrix_b)\n",
        "\n",
        "        image3 = pcv.transform.apply_transformation_matrix(source_img = image, target_img = cc_img, transformation_matrix = transformation_matrix)\n",
        "\n",
        "    else:\n",
        "        image3 = image\n",
        "\n",
        "    ###\n",
        "    # Use the FCN model to predict berry pixels\n",
        "    ###\n",
        "\n",
        "    # Create a tensor from the image\n",
        "    image3 = image3.astype(\"float\")\n",
        "    new_im = np.zeros((3, newH, newW))\n",
        "    new_im[0,:,:] = image3[:,:,0]\n",
        "    new_im[1,:,:] = image3[:,:,1]\n",
        "    new_im[2,:,:] = image3[:,:,2]\n",
        "    image_tensor = new_im\n",
        "    image_tensor = torch.from_numpy(image_tensor)\n",
        "    # Normalize the tensor and send it to the GPU\n",
        "    image_tensor = T.Normalize(mean=mean, std=std)(image_tensor)\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    image_tensor = image_tensor.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    # Run the image through the prediction model\n",
        "    with torch.no_grad():\n",
        "        mask = model(image_tensor)['out']\n",
        "        mask = nn.Sigmoid()(mask)\n",
        "        mask = mask.cpu().detach().numpy()\n",
        "\n",
        "    ###\n",
        "    # Measure berry properties\n",
        "    ###\n",
        "\n",
        "    # Iterate over materials to print\n",
        "    for mat_to_print in materials_toprint:\n",
        "\n",
        "        # Find the index of this material in the materials list\n",
        "        mat_idx = [i for i, x in enumerate(materials) if x.name == mat_to_print][0]\n",
        "        # Get the material at this index\n",
        "        mat = materials[mat_idx]\n",
        "        # Get the mask from the prediction model at this index\n",
        "        mat_mask = mask[0,mat_idx,:,:]\n",
        "        mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
        "        mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
        "\n",
        "        # Perform object segmentation and regionprop calculation\n",
        "        # This is from https://github.com/danforthcenter/plantcv/blob/master/plantcv/plantcv/watershed.py\n",
        "        # Convert the mat_mask to 8-bit\n",
        "        mat_mask = mat_mask.astype(\"uint8\")\n",
        "\n",
        "        # Run watershed here? Or binary erosion?\n",
        "        # For now, skip\n",
        "        # Run distance transform\n",
        "        # dist_transform = cv.distanceTransformWithLabels(mat_mask, distanceType = cv.DIST_L2, maskSize = 0)[0]\n",
        "        # local_max = feature.peak_local_max(dist_transform, indices = False, min_distance = distance, labels = mat_mask)\n",
        "        # markers = ndi.label(local_max, structure=np.ones((3, 3)))[0]\n",
        "        # dist_transform1 = -dist_transform\n",
        "        # seg1 = segmentation.watershed(dist_transform1, markers, mask = mat_mask)\n",
        "        seg1 = mat_mask\n",
        "\n",
        "        ## Estimate berry traits\n",
        "        # Label the segmentation output\n",
        "        label_mat = label(np.array(seg1), background = 0)\n",
        "\n",
        "        # Regionprops\n",
        "        region_properties1 = list(set([\"label\", \"bbox\"] + region_properties))\n",
        "        region_properties_names = region_properties1 + [x + \"_intensity_mean\" for x in [\"red\", \"green\", \"blue\"]] + [x + \"_intensity_sd\" for x in [\"red\", \"green\", \"blue\"]]\n",
        "        region_properties_names = region_properties_names + [x + \"_intensity_mean\" for x in [\"hue\", \"sat\", \"val\"]] + [x + \"_intensity_sd\" for x in [\"hue\", \"sat\", \"val\"]]\n",
        "        region_properties_names = tuple([\"file_name\", \"collection_id\", \"color_corrected\", \"material\", \"label\"] + [x for x in region_properties_names if x != \"label\"])\n",
        "\n",
        "        # Empty dictionary to store data\n",
        "        regions_dict = {}\n",
        "\n",
        "        # Initialize lists in the dictionary\n",
        "        for key in region_properties_names:\n",
        "            regions_dict[key] = []\n",
        "\n",
        "        # Iterate over regions in the image\n",
        "        for region in regionprops(label_image = label_mat):\n",
        "\n",
        "            # Add manual keys\n",
        "            regions_dict[\"file_name\"] = filename\n",
        "            regions_dict[\"collection_id\"] = cid\n",
        "            regions_dict[\"color_corrected\"] = str(color_card_found)\n",
        "            regions_dict[\"material\"] = mat_to_print\n",
        "\n",
        "            # Add props to the dictionary\n",
        "            for prop in region_properties1:\n",
        "                regions_dict[prop].append(region[prop])\n",
        "\n",
        "\n",
        "            # Convert image to HSV\n",
        "            image3_hsv = cv.cvtColor(image3.astype(\"uint8\"), cv.COLOR_RGB2HSV)\n",
        "\n",
        "            berry_rgb_values = []\n",
        "            berry_hsv_values = []\n",
        "\n",
        "            for y, x in region.coords:\n",
        "                berry_rgb_values.append(image3[y, x, :])\n",
        "                berry_hsv_values.append(image3_hsv[y, x, :])\n",
        "\n",
        "            for c, left in enumerate([\"red\", \"green\", \"blue\"]):\n",
        "                key = left + \"_intensity_mean\"\n",
        "                vals = [x[c] for x in berry_rgb_values]\n",
        "                regions_dict[key].append(np.mean(vals))\n",
        "\n",
        "                key = key.replace(\"mean\", \"sd\")\n",
        "                regions_dict[key].append(np.std(vals))\n",
        "\n",
        "            for c, left in enumerate([\"hue\", \"sat\", \"val\"]):\n",
        "                key = left + \"_intensity_mean\"\n",
        "                vals = [x[c] for x in berry_hsv_values]\n",
        "                regions_dict[key].append(np.mean(vals))\n",
        "\n",
        "                key = key.replace(\"mean\", \"sd\")\n",
        "                regions_dict[key].append(np.std(vals))\n",
        "\n",
        "        # Convert the regions_dict to a data.frame\n",
        "        regions_df = pd.DataFrame(regions_dict)\n",
        "\n",
        "        # Remove excessively large regions\n",
        "        regions_df = regions_df[(regions_df[\"area\"] <= max_area) & (regions_df[\"area\"] >= min_area)]\n",
        "\n",
        "        # Convert area, length, width to cm\n",
        "        regions_df[\"area\"] = regions_df[\"area\"] * cm2_per_pixel\n",
        "        regions_df[\"major_axis_length\"] = regions_df[\"major_axis_length\"] * cm_per_pixel\n",
        "        regions_df[\"minor_axis_length\"] = regions_df[\"minor_axis_length\"] * cm_per_pixel\n",
        "\n",
        "        # Save the region data\n",
        "        all_region_df.append(regions_df)\n",
        "\n",
        "        ### Save a segmentation image ###\n",
        "        if save_segmentation_image:\n",
        "            image_use = image3.astype(\"uint8\")\n",
        "            image_use = label2rgb(label_mat, image_use, alpha=0.3, bg_label = 0)\n",
        "            regions_df_use = regions_df.to_dict()\n",
        "\n",
        "            # Iterate over the berry index\n",
        "            for lab in regions_df_use[\"label\"]:\n",
        "                bbox = regions_df_use[\"bbox\"][lab]\n",
        "                # draw rectangle around segmented coins\n",
        "                minr, minc, maxr, maxc = bbox\n",
        "                cv.rectangle(image_use, (minc, minr), (maxc, maxr),(0,255,0),2)\n",
        "\n",
        "            image_use_save = Image.fromarray((image_use * 255).astype(\"uint8\"))\n",
        "            image_use_save.save(seg_output_directory + \"/\" + mat_to_print + \"-segmented-\" + cid + \"-\" + filename)\n",
        "\n",
        "# Save the region data\n",
        "# Merge the region data.feames\n",
        "all_regions_data = pd.concat(all_region_df)\n",
        "# Drop the bbox column\n",
        "all_regions_data = all_regions_data.drop(columns = \"bbox\")\n",
        "region_filename = output_directory + \"/\" + current_model_name + \"_InferenceImageRegionData.csv\"\n",
        "all_regions_data.to_csv(region_filename, index = False)\n",
        "            \n",
        "print(\"\\nImage analysis pipeline complete!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7Eu20-BUkAaP"
      },
      "id": "7Eu20-BUkAaP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deploy_BerryBox_FCNSegmentationModel_Colab.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3766c16f77b246db91426c6c42c3d60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e2c44b2667f4673839ece1322bb76ee",
              "IPY_MODEL_2af316247e8a46dbbabb97fe418f65d3",
              "IPY_MODEL_06ea2f9876f6405dad8f75919f762c32"
            ],
            "layout": "IPY_MODEL_b3d14fde36944240a591e09134306add"
          }
        },
        "8e2c44b2667f4673839ece1322bb76ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e42f673d2214b0783dfa096d182fab9",
            "placeholder": "​",
            "style": "IPY_MODEL_84603b4865ed4b47aa64c6f3739d82cc",
            "value": "100%"
          }
        },
        "2af316247e8a46dbbabb97fe418f65d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c99fea21ce9a40c8a306213f6c8b8d34",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47dce6ff41a74366a62734a5ff83a725",
            "value": 178793939
          }
        },
        "06ea2f9876f6405dad8f75919f762c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_322ecdd18ee748e58c60768b0af5827b",
            "placeholder": "​",
            "style": "IPY_MODEL_39c01703fdec4c349c538c80cdb69586",
            "value": " 171M/171M [00:03&lt;00:00, 63.4MB/s]"
          }
        },
        "b3d14fde36944240a591e09134306add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e42f673d2214b0783dfa096d182fab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84603b4865ed4b47aa64c6f3739d82cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c99fea21ce9a40c8a306213f6c8b8d34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47dce6ff41a74366a62734a5ff83a725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "322ecdd18ee748e58c60768b0af5827b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c01703fdec4c349c538c80cdb69586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}