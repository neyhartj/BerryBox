{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neyhartj/BerryBox/blob/master/deploy_BerryBox_FCNSegmentationModel_Colab_20220629.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BerryBox Image Analysis Pipeline\n",
        "\n",
        "Use this notebook for production applications of a trained fully convolutional network (FCN) to measure quality parameters on individual berries in images.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Prior to running the pipeline, make sure you have completed these setup steps:\n",
        "\n",
        "1. Clone the "
      ],
      "metadata": {
        "id": "1RJFn5hLaSrw"
      },
      "id": "1RJFn5hLaSrw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_7FuYyMej4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430c4eef-9838-4814-c268-5bfb42629a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Tue Jun 14 15:04:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Mount \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "drive = \"/content/drive/MyDrive\"\n",
        "\n",
        "!nvidia-smi"
      ],
      "id": "f_7FuYyMej4V"
    },
    {
      "cell_type": "markdown",
      "id": "4d076506",
      "metadata": {
        "id": "4d076506"
      },
      "source": [
        "\n",
        "# **Materials**\n",
        "  Input the material mask name and information below. Some of the items described here may not appear below, but anything that appears below is described here.\n",
        "\n",
        "  Specifically:\n",
        " \n",
        "  **name** - The name for the material. This is pretty arbitrary, but it will be\n",
        "  used to label output folders and images.\n",
        " \n",
        "  **input_rbg_vals** - The rbg values of the material in the input mask image.\n",
        " \n",
        "  **output_val** - The greyscale value of the mask when you output the images.\n",
        "  This is arbitrary, but every material should have its own output color\n",
        "  so they can be differentiated\n",
        " \n",
        "  **confidence_threshold** - The lower this number, the more voxels will be labled a specific material. Essentially, the ML algorith outptus a confdience value  (centered on 0.5) for every voxel and every material. By default, voxels with  a confidence of 0.5 or greater are determined to be the material in question.  But we can labled voxles with a lower condience level by changing this  parameter\n",
        "\n",
        "  **training_image_directory /training_mask_directory**: Input the directory where your training images and masks are located.\n",
        "\n",
        "  **validation_fraction**: Input the fraction of images you want to validate your model during training. These are not a independent validation, but are part of the training process.\n",
        "\n",
        "  **num_models**: Enter the number of models you want to iteratively train. Because these are statistical models, the performance of any given model will vary. Training more models will allow you to select the model that best fits your data.\n",
        "  \n",
        "  **num_epochs**: Enter number of epochs that you want to use to train your model. More is generally better, but takes more time.\n",
        "\n",
        "  **batch_size**: Input your batch size. Larger batch sizes allow for faster training, but take up more VRAM. If you are running out of VRAM during training, decrease your batch size.\n",
        "\n",
        "  **scale**: Input how you want your images scaled during model training and inference. When the scale is 1, your images will be used at full size for training. When the scale is less than 1, your images will be downsized according to the scale you set for training and inference, decreasing VRAM usage. If you run out of VRAM during training, consider rescaling your images.\n",
        "  \n",
        "  **normalization_path**: The path to the normalization data file that was saved during model training.\n",
        "\n",
        "  **models_directory**: Directory where your models are saved.\n",
        "\n",
        "  **model_group**: Name for the group models you iteratively generate.\n",
        "\n",
        "  **current_model_name**: Name for each individual model you generate; will automatically be labeled 1 through n for the number of models you specify above.\n",
        "\n",
        "  **val_images/val_masks**: Input the directory where your independent validation images and masks are located. These images are not used for training and are used as an independent validation of your model.\n",
        "\n",
        "  **csv_directory**: Directory where a CSV file of your validation results will be saved.\n",
        "\n",
        "  **inference_directory**: Directory where the images you want analyzed are located.\n",
        "\n",
        "  **output_directory**: Directory where you want your analysis results to be saved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-a6Oocjd2Wu-",
      "metadata": {
        "id": "-a6Oocjd2Wu-"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "#### Set user parameters ####\n",
        "#############################\n",
        "\n",
        "class Material:\n",
        " \n",
        "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
        "    self.name = name\n",
        "    self.input_rgb_vals = input_rgb_vals\n",
        "    self.output_val = output_val\n",
        "    self.confidence_threshold = confidence_threshold\n",
        "\n",
        "#Creating a list of materials so we can iterate through it\n",
        "materials = [\n",
        "             Material(\"background\", [0,0,0], 0, 0.5),\n",
        "             Material(\"berry\", [255,255,255], 255, 0.75),\n",
        "             ]\n",
        "\n",
        "\n",
        "# What material would you like to make inferences for?\n",
        "materials_toprint = [\"berry\"]\n",
        "\n",
        "# Project directory\n",
        "# IMPORTANT - ALL DIRECTORIES NEED TO END IN A /\n",
        "proj_dir =  drive + \"/ARS_Cranberry/ImageAnalysis/BerryBox/2021_BerryBox_ImageAnalysis/\"\n",
        "\n",
        "# Distance for the watershed segmentation\n",
        "distance = 10\n",
        "\n",
        "# Maximum object area (in pixels) to keep\n",
        "max_area = 15000\n",
        "# Minimum object area (in pixels) to keep\n",
        "min_area = 600\n",
        "\n",
        "# Properties for regionprops\n",
        "# region_properties = [\"area\", \"axis_major_length\", \"axis_minor_length\", \"eccentricity\"] # For local runs\n",
        "region_properties = [\"area\", \"major_axis_length\", \"minor_axis_length\", \"eccentricity\"] # For colab runs\n",
        "\n",
        "# Should the pipeline include watershed segmentation? Note: this can be unreliable\n",
        "run_watershed = False\n",
        "\n",
        "# Should the pipeline save segmented images\n",
        "save_segmentation_image = True\n",
        "\n",
        "# Input deep learning model path \n",
        "# This file should end in \".pth\"\n",
        "model_path = drive + \"/ARS_Cranberry/ImageAnalysis/BerryBox/fcn_model_building/model_output/berryBox_fcn_0.0.3/models/berryBox_fcn_0.0.3_model3.pth\"\n",
        "\n",
        "# Normalization data path\n",
        "normalization_path = drive + \"/ARS_Cranberry/ImageAnalysis/BerryBox/fcn_model_building/model_output/berryBox_fcn_0.0.3/berryBox_fcn_0.0.3_normalization_param.txt\"\n",
        "\n",
        "# Directory of images to segment\n",
        "inference_directory = proj_dir + \"/imagesToSegment\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X49s7N2WXHKM",
      "metadata": {
        "id": "X49s7N2WXHKM"
      },
      "source": [
        "# **Image Segmentation**\n",
        "\n",
        "Run the image inference pipeline. This pipeline will:\n",
        "1. Read in an inference image and identify the QR code and scaling\n",
        "2. Run the image through the prediction model\n",
        "3. Segment the relevant mask\n",
        "4. Identify objects in the image\n",
        "5. Measure object properties and save the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-MuczokV7w-A",
      "metadata": {
        "id": "-MuczokV7w-A"
      },
      "source": [
        "## Import packages and load a specific model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nPEGUMpy7zp_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a89c2c5ddb5e4777ab602af416f26fa7",
            "ab2c4a2a30934a4bbe1f13c7cf60093d",
            "df88c3dc93904f3e8969c4568e6975e8",
            "e577876efc9e46b7b9fa3c030b57f462",
            "c67f412e10444de5a3ded9aa37de7eb0",
            "f141cd5ea7a347ca8a5aac76220f7bbe",
            "cbe58d5c7510487fa14f0ad7135197c6",
            "49bc96294f6b452bb5009f40ca85fe2b",
            "bb3f41c070b44aa59b5dbf57f4238624",
            "7951794b931d4772b1a947852d31de76",
            "bdca77c235bd4ce899896b623f05927a"
          ]
        },
        "id": "nPEGUMpy7zp_",
        "outputId": "85e5a7a5-0435-490d-ba54-5737fb145d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a89c2c5ddb5e4777ab602af416f26fa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load relevant packages\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from torchvision.models.segmentation.fcn import FCNHead\n",
        "from torchvision.models.segmentation import fcn_resnet101\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from scipy import ndimage as ndi\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from skimage.color import rgb2gray, label2rgb\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from skimage import feature, segmentation\n",
        "from skimage.measure import label, regionprops_table, regionprops\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Name of the model group\n",
        "current_model_name = os.path.basename(model_path).replace(\".pth\", \"\")\n",
        "model_group = current_model_name.split(\"_model\")[0]\n",
        "\n",
        "\n",
        "## Specify directories\n",
        "# Directory to store output segmented images\n",
        "output_directory = proj_dir + \"/segmentation_output\"\n",
        "# Directory of segmented images\n",
        "seg_output_directory = output_directory + \"/segmented_images\"\n",
        "\n",
        "# Empty and create these directories\n",
        "for dirname in [output_directory, seg_output_directory]:\n",
        "    if not os.path.exists(dirname):\n",
        "        os.mkdir(dirname)\n",
        "\n",
        "# How many materials?\n",
        "num_materials = len(materials)\n",
        "\n",
        "\n",
        "\n",
        "# Load a pretrained model\n",
        "model = fcn_resnet101(pretrained=False)\n",
        "model.classifier=FCNHead(2048, num_materials)\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        " \n",
        "# Load the model specified above\n",
        "model.load_state_dict(torch.load(model_path), strict=False)\n",
        "model.train()\n",
        "\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "\n",
        "## Load the normalization information ##\n",
        "# Read in the important model training log information\n",
        "with open(normalization_path, \"r\") as file:\n",
        "    for line in file:\n",
        "        # Strip off newline; separate by tab\n",
        "        tabs = line.strip().split(\"\\t\")\n",
        "\n",
        "        # Get the name of the variable; this will be used for assignment\n",
        "        var_name = tabs[0]\n",
        "\n",
        "        # Parse the second tab\n",
        "        if tabs[1].startswith(\"tensor\"):\n",
        "\n",
        "            # Create a vector of numeric characters\n",
        "            var_value = tabs[1].split(\"[\")[1].split(\"]\")[0].split(\", \")\n",
        "            # Convert this to numeric\n",
        "            var_value = [float(x) for x in var_value]\n",
        "            # Convert to np array; then to tensor\n",
        "            var_value = np.array(var_value)\n",
        "            var_value = torch.tensor(var_value)\n",
        "\n",
        "        else:\n",
        "            var_value = float(tabs[1])\n",
        "\n",
        "        # Assign variable name\n",
        "        vars()[var_name] = var_value\n",
        "        \n",
        "# assign to mean and std\n",
        "mean = normalization_mean\n",
        "std = normalization_std\n",
        "newW = int(image_scale_newW)\n",
        "newH = int(image_scale_newH)\n",
        "\n",
        "\n",
        "# Load a QR code detector\n",
        "detector = cv.QRCodeDetector()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S7qVnO9B8IK-",
      "metadata": {
        "id": "S7qVnO9B8IK-"
      },
      "source": [
        "## Run the image processing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FDWEvRgsy_qC",
      "metadata": {
        "id": "FDWEvRgsy_qC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c688cd67-7ac2-417d-9b8b-47adb0cfe2e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "861 images found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "861it [45:35,  3.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation complete!\n"
          ]
        }
      ],
      "source": [
        "## Iterate over images and run through the prediction model ##\n",
        "\n",
        "# Rename the directory containing the images to segment\n",
        "dir_name = inference_directory\n",
        "filenames = os.listdir(dir_name)\n",
        "print(str(len(filenames)) + \" images found.\")\n",
        "\n",
        "# Create an empty list to store region data\n",
        "all_region_df = []\n",
        "\n",
        "# Iterate over the images\n",
        "for i, filename in tqdm(enumerate(filenames)):\n",
        "\n",
        "# ## TESTING ##\n",
        "# i = 0\n",
        "# filename = filenames[i]\n",
        "\n",
        "    # Open the image\n",
        "    image = Image.open(dir_name +'/'+ filename)\n",
        "    image_cv = cv.imread(dir_name + \"/\" + filename)\n",
        "\n",
        "    # Rescale the image\n",
        "    image = image.resize((newW, newH))\n",
        "    # Convert to gray\n",
        "    image_gray = np.asarray(image)\n",
        "    image_gray = rgb2gray(image_gray)\n",
        "    # image_gray = image_gray[0:newH, 0:newW] # Add this to resize image1\n",
        "    image = np.array(image, dtype = float)\n",
        "    new_im = np.zeros((3, newH, newW))\n",
        "    new_im[0,:,:] = image[:,:,0]\n",
        "    new_im[1,:,:] = image[:,:,1]\n",
        "    new_im[2,:,:] = image[:,:,2]\n",
        "    image_tensor = new_im\n",
        "\n",
        "\n",
        "    # Create a tensor from the image\n",
        "    image_tensor = torch.from_numpy(image_tensor)\n",
        "    # Normalize the tensor and send it to the GPU\n",
        "    image_tensor = T.Normalize(mean=mean, std=std)(image_tensor)\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    image_tensor = image_tensor.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    # Run the image through the prediction model\n",
        "    with torch.no_grad():\n",
        "        mask = model(image_tensor)['out']\n",
        "        mask = nn.Sigmoid()(mask)\n",
        "        mask = mask.cpu().detach().numpy()\n",
        "\n",
        "    # Iterate over materials to print\n",
        "    for mat_to_print in materials_toprint:\n",
        "        # Find the index of this material in the materials list\n",
        "        mat_idx = [i for i, x in enumerate(materials) if x.name == mat_to_print][0]\n",
        "\n",
        "        # Get the material at this index\n",
        "        mat = materials[mat_idx]\n",
        "\n",
        "        # Get the mask from the prediction model at this index\n",
        "        mat_mask = mask[0,mat_idx,:,:]\n",
        "        mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
        "        mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
        "\n",
        "        # Perform object segmentation and regionprop calculation\n",
        "        # This is from https://github.com/danforthcenter/plantcv/blob/master/plantcv/plantcv/watershed.py\n",
        "        # Convert the mat_mask to 8-bit\n",
        "        mat_mask = mat_mask.astype(\"uint8\")\n",
        "\n",
        "        # Run the watershed if called\n",
        "        if run_watershed:\n",
        "            # Run distance transform\n",
        "            dist_transform = cv.distanceTransformWithLabels(mat_mask, distanceType = cv.DIST_L2, maskSize = 0)[0]\n",
        "            local_max = feature.peak_local_max(dist_transform, indices = False, min_distance = distance, labels = mat_mask)\n",
        "\n",
        "            markers = ndi.label(local_max, structure=np.ones((3, 3)))[0]\n",
        "            dist_transform1 = -dist_transform\n",
        "            seg1 = segmentation.watershed(dist_transform1, markers, mask = mat_mask)\n",
        "        else:\n",
        "            seg1 = mat_mask\n",
        "\n",
        "        ## Estimate berry traits\n",
        "        # Label the segmentation output\n",
        "        label_mat = label(np.array(seg1), background = 0)\n",
        "\n",
        "        # Regionprops\n",
        "        region_properties1 = list(set([\"label\", \"bbox\"] + region_properties))\n",
        "        region_properties_names = region_properties1 + [x + \"_intensity_mean\" for x in [\"red\", \"green\", \"blue\"]] + [x + \"_intensity_sd\" for x in [\"red\", \"green\", \"blue\"]]\n",
        "        region_properties_names = region_properties_names + [x + \"_intensity_mean\" for x in [\"hue\", \"sat\", \"val\"]] + [x + \"_intensity_sd\" for x in [\"hue\", \"sat\", \"val\"]]\n",
        "        region_properties_names = tuple([\"file_name\", \"material\", \"label\"] + [x for x in region_properties_names if x != \"label\"])\n",
        "\n",
        "        # Empty dictionary to store data\n",
        "        regions_dict = {}\n",
        "\n",
        "        # Initialize lists in the dictionary\n",
        "        for key in region_properties_names:\n",
        "            regions_dict[key] = []\n",
        "\n",
        "        # Iterate over regions in the image\n",
        "        for region in regionprops(label_image = label_mat):\n",
        "\n",
        "            # Add manual keys\n",
        "            regions_dict[\"file_name\"] = filename\n",
        "            regions_dict[\"material\"] = mat_to_print\n",
        "\n",
        "            # Add props to the dictionary\n",
        "            for prop in region_properties1:\n",
        "                regions_dict[prop].append(region[prop])\n",
        "\n",
        "\n",
        "            # Convert image to HSV\n",
        "            image_hsv = cv.cvtColor(image.astype(\"uint8\"), cv.COLOR_RGB2HSV)\n",
        "\n",
        "            berry_rgb_values = []\n",
        "            berry_hsv_values = []\n",
        "\n",
        "            for y, x in region.coords:\n",
        "                berry_rgb_values.append(image[y, x, :])\n",
        "                berry_hsv_values.append(image_hsv[y, x, :])\n",
        "\n",
        "            for c, left in enumerate([\"red\", \"green\", \"blue\"]):\n",
        "                key = left + \"_intensity_mean\"\n",
        "                vals = [x[c] for x in berry_rgb_values]\n",
        "                regions_dict[key].append(np.mean(vals))\n",
        "\n",
        "                key = key.replace(\"mean\", \"sd\")\n",
        "                regions_dict[key].append(np.std(vals))\n",
        "\n",
        "            for c, left in enumerate([\"hue\", \"sat\", \"val\"]):\n",
        "                key = left + \"_intensity_mean\"\n",
        "                vals = [x[c] for x in berry_hsv_values]\n",
        "                regions_dict[key].append(np.mean(vals))\n",
        "\n",
        "                key = key.replace(\"mean\", \"sd\")\n",
        "                regions_dict[key].append(np.std(vals))\n",
        "\n",
        "        # Convert the regions_dict to a data.frame\n",
        "        regions_df = pd.DataFrame(regions_dict)\n",
        "\n",
        "        # Remove excessively large regions\n",
        "        regions_df1 = regions_df[(regions_df[\"area\"] <= max_area) & (regions_df[\"area\"] >= min_area)]\n",
        "        \n",
        "        # Save the region data\n",
        "        all_region_df.append(regions_df1)\n",
        "\n",
        "        ### Save a segmentation image ###\n",
        "        if save_segmentation_image:\n",
        "            image_use = image.astype(\"uint8\")\n",
        "            image_use = label2rgb(label_mat, image_use, alpha=0.3, bg_label = 0)\n",
        "            regions_df_use = regions_df1.to_dict()\n",
        "\n",
        "            # Iterate over the berry index\n",
        "            for lab in regions_df_use[\"label\"]:\n",
        "                bbox = regions_df_use[\"bbox\"][lab]\n",
        "                # draw rectangle around segmented coins\n",
        "                minr, minc, maxr, maxc = bbox\n",
        "                cv.rectangle(image_use, (minc, minr), (maxc, maxr),(0,255,0),2)\n",
        "\n",
        "            image_use_save = Image.fromarray((image_use * 255).astype(\"uint8\"))\n",
        "            image_use_save.save(seg_output_directory + \"/\" + mat_to_print + \"-segmented-\" + filename)\n",
        "            \n",
        "\n",
        "# Save the region data\n",
        "# Merge the region data.feames\n",
        "all_regions_data = pd.concat(all_region_df)\n",
        "region_filename = output_directory + \"/\" + current_model_name + \"_InferenceImageRegionData.csv\"\n",
        "all_regions_data.to_csv(region_filename)\n",
        "            \n",
        "print(\"Segmentation complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deploy_BerryBox_FCNSegmentationModel_Colab_20220629.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "virtenv_cuda113",
      "language": "python",
      "name": "virtenv_cuda113"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a89c2c5ddb5e4777ab602af416f26fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab2c4a2a30934a4bbe1f13c7cf60093d",
              "IPY_MODEL_df88c3dc93904f3e8969c4568e6975e8",
              "IPY_MODEL_e577876efc9e46b7b9fa3c030b57f462"
            ],
            "layout": "IPY_MODEL_c67f412e10444de5a3ded9aa37de7eb0"
          }
        },
        "ab2c4a2a30934a4bbe1f13c7cf60093d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f141cd5ea7a347ca8a5aac76220f7bbe",
            "placeholder": "​",
            "style": "IPY_MODEL_cbe58d5c7510487fa14f0ad7135197c6",
            "value": "100%"
          }
        },
        "df88c3dc93904f3e8969c4568e6975e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49bc96294f6b452bb5009f40ca85fe2b",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb3f41c070b44aa59b5dbf57f4238624",
            "value": 178793939
          }
        },
        "e577876efc9e46b7b9fa3c030b57f462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7951794b931d4772b1a947852d31de76",
            "placeholder": "​",
            "style": "IPY_MODEL_bdca77c235bd4ce899896b623f05927a",
            "value": " 171M/171M [00:00&lt;00:00, 219MB/s]"
          }
        },
        "c67f412e10444de5a3ded9aa37de7eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f141cd5ea7a347ca8a5aac76220f7bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbe58d5c7510487fa14f0ad7135197c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49bc96294f6b452bb5009f40ca85fe2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3f41c070b44aa59b5dbf57f4238624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7951794b931d4772b1a947852d31de76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdca77c235bd4ce899896b623f05927a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}