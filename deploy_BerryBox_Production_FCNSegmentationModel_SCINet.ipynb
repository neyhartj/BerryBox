{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d076506",
   "metadata": {
    "id": "4d076506"
   },
   "source": [
    "# BerryBox Image Analysis Pipeline\n",
    "\n",
    "\n",
    "## Validation only\n",
    "\n",
    "This notebook provides the code to validate a developmental FCN model. **It is not meant for production use.**\n",
    "\n",
    "\n",
    "# **Materials**\n",
    "  Input the material mask name and information below. Some of the items described here may not appear below, but anything that appears below is described here.\n",
    "\n",
    "  Specifically:\n",
    " \n",
    "  **name** - The name for the material. This is pretty arbitrary, but it will be\n",
    "  used to label output folders and images.\n",
    " \n",
    "  **input_rbg_vals** - The rbg values of the material in the input mask image.\n",
    " \n",
    "  **output_val** - The greyscale value of the mask when you output the images.\n",
    "  This is arbitrary, but every material should have its own output color\n",
    "  so they can be differentiated\n",
    " \n",
    "  **confidence_threshold** - The lower this number, the more voxels will be labled a specific material. Essentially, the ML algorith outptus a confdience value  (centered on 0.5) for every voxel and every material. By default, voxels with  a confidence of 0.5 or greater are determined to be the material in question.  But we can labled voxles with a lower condience level by changing this  parameter\n",
    "\n",
    "  **training_image_directory /training_mask_directory**: Input the directory where your training images and masks are located.\n",
    "\n",
    "  **validation_fraction**: Input the fraction of images you want to validate your model during training. These are not a independent validation, but are part of the training process.\n",
    "\n",
    "  **num_models**: Enter the number of models you want to iteratively train. Because these are statistical models, the performance of any given model will vary. Training more models will allow you to select the model that best fits your data.\n",
    "  \n",
    "  **num_epochs**: Enter number of epochs that you want to use to train your model. More is generally better, but takes more time.\n",
    "\n",
    "  **batch_size**: Input your batch size. Larger batch sizes allow for faster training, but take up more VRAM. If you are running out of VRAM during training, decrease your batch size.\n",
    "\n",
    "  **scale**: Input how you want your images scaled during model training and inference. When the scale is 1, your images will be used at full size for training. When the scale is less than 1, your images will be downsized according to the scale you set for training and inference, decreasing VRAM usage. If you run out of VRAM during training, consider rescaling your images.\n",
    "  \n",
    "  **normalization_path**: The path to the normalization data file that was saved during model training.\n",
    "\n",
    "  **models_directory**: Directory where your models are saved.\n",
    "\n",
    "  **model_group**: Name for the group models you iteratively generate.\n",
    "\n",
    "  **current_model_name**: Name for each individual model you generate; will automatically be labeled 1 through n for the number of models you specify above.\n",
    "\n",
    "  **val_images/val_masks**: Input the directory where your independent validation images and masks are located. These images are not used for training and are used as an independent validation of your model.\n",
    "\n",
    "  **csv_directory**: Directory where a CSV file of your validation results will be saved.\n",
    "\n",
    "  **inference_directory**: Directory where the images you want analyzed are located.\n",
    "\n",
    "  **output_directory**: Directory where you want your analysis results to be saved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "-a6Oocjd2Wu-",
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1654800436883,
     "user": {
      "displayName": "Jeffrey Neyhart",
      "userId": "07720342469706758133"
     },
     "user_tz": 240
    },
    "id": "-a6Oocjd2Wu-"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "#### Set user parameters ####\n",
    "#############################\n",
    "\n",
    "class Material:\n",
    " \n",
    "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
    "    self.name = name\n",
    "    self.input_rgb_vals = input_rgb_vals\n",
    "    self.output_val = output_val\n",
    "    self.confidence_threshold = confidence_threshold\n",
    "\n",
    "#Creating a list of materials so we can iterate through it\n",
    "materials = [\n",
    "             Material(\"background\", [0,0,0], 0, 0.5),\n",
    "             Material(\"berry\", [255,255,255], 255, 0.75),\n",
    "             ]\n",
    "\n",
    "\n",
    "# What material would you like to make inferences for?\n",
    "materials_toprint = [\"berry\"]\n",
    "\n",
    "# Project directory\n",
    "# IMPORTANT - ALL DIRECTORIES NEED TO END IN A /\n",
    "# proj_dir =  drive + \"/ARS_Cranberry/ImageAnalysis/BerryBox/fcn_model_building/\"\n",
    "proj_dir = \"/project/gifvl_vaccinium/cranberryImaging/BerryBox/fcn_model_building/\"\n",
    "\n",
    "\n",
    "#Decrease scale to decrease VRAM usage; if you run out of VRAM during traing, restart your runtime and down scale your images\n",
    "scale = 0.3\n",
    "\n",
    "# Distance for the watershed segmentation\n",
    "distance = 10\n",
    "\n",
    "# Properties for regionprops\n",
    "# region_properties = [\"area\", \"axis_major_length\", \"axis_minor_length\", \"eccentricity\"] # For local runs\n",
    "region_properties = [\"area\", \"major_axis_length\", \"minor_axis_length\", \"eccentricity\"] # For colab runs\n",
    "\n",
    "# Should the pipeline include watershed segmentation? Note: this can be unreliable\n",
    "run_watershed = False\n",
    "\n",
    "# Input deep learning model path \n",
    "# This file should end in \".pth\"\n",
    "model_path = proj_dir + \"/model_output/berryBox_fcn_0.0.2/models/berryBox_fcn_0.0.2_model3.pth\"\n",
    "\n",
    "# Normalization data path\n",
    "normalization_path = proj_dir + \"/model_output/berryBox_fcn_0.0.2/berryBox_fcn_20220608-113848_model_normalization_param.txt\"\n",
    "\n",
    "# Name of the model group\n",
    "model_group = \"berryBox_fcn_0.0.2/\"\n",
    "current_model_name = model_group.replace(\"/\", \"\") + \"_model\"\n",
    "\n",
    "# Directory of images to segment\n",
    "inference_directory = proj_dir + \"/imagesToSegment\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X49s7N2WXHKM",
   "metadata": {
    "id": "X49s7N2WXHKM"
   },
   "source": [
    "# **Image Segmentation**\n",
    "\n",
    "Run the image inference pipeline. This pipeline will:\n",
    "1. Read in an inference image and identify the QR code and scaling\n",
    "2. Run the image through the prediction model\n",
    "3. Segment the relevant mask\n",
    "4. Identify objects in the image\n",
    "5. Measure object properties and save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-MuczokV7w-A",
   "metadata": {
    "id": "-MuczokV7w-A"
   },
   "source": [
    "## Import packages and load a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nPEGUMpy7zp_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3168,
     "status": "ok",
     "timestamp": 1654802445815,
     "user": {
      "displayName": "Jeffrey Neyhart",
      "userId": "07720342469706758133"
     },
     "user_tz": 240
    },
    "id": "nPEGUMpy7zp_",
    "outputId": "af09db98-c817-450c-d5a8-980a422e21e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load relevant packages\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation import fcn_resnet101\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from scipy import ndimage as ndi\n",
    "import pandas as pd\n",
    "from skimage.color import rgb2gray, label2rgb\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import feature, segmentation\n",
    "from skimage.measure import label, regionprops_table, regionprops\n",
    "from plantcv import plantcv as pcv\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Specify directories\n",
    "# Directory to store output segmented images\n",
    "output_directory = proj_dir + \"/segmentation_output\"\n",
    "# Directory of segmented images\n",
    "seg_output_directory = output_directory + \"/segmented_images\"\n",
    "\n",
    "# Empty and create these directories\n",
    "for dirname in [output_directory, seg_output_directory]:\n",
    "  if os.path.exists(dirname):\n",
    "    # If exists, delete everything within it\n",
    "    for subfile in os.listdir(dirname):\n",
    "      if os.path.isdir(dirname + \"/\" + subfile):\n",
    "        continue\n",
    "      else:\n",
    "        os.remove(dirname + \"/\" + subfile)\n",
    "  else:\n",
    "    # Else create\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "# How many materials?\n",
    "num_materials = len(materials)\n",
    "\n",
    "# Load a pretrained model\n",
    "model = fcn_resnet101(pretrained=False)\n",
    "model.classifier=FCNHead(2048, num_materials)\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    " \n",
    "# Load the model specified above\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "model.train()\n",
    "\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "\n",
    "## Load the normalization information ##\n",
    "# Empty dict to store tensors\n",
    "norm_tensors = {}\n",
    "\n",
    "# Read in the normalization file\n",
    "with open(normalization_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        tabs = line.split(\"\\t\")\n",
    "        \n",
    "        # Create a vector of numeric characters\n",
    "        num_char_vec = tabs[1].split(\"[\")[1].split(\"]\")[0].split(\", \")\n",
    "        # Convert this to numeric\n",
    "        num_vec = [float(x) for x in num_char_vec]\n",
    "        # Convert to np array\n",
    "        num_arr = np.array(num_vec)\n",
    "        \n",
    "        # Convert to tensor and store\n",
    "        norm_tensors[tabs[0]] = torch.tensor(num_arr)\n",
    "        \n",
    "# assign to mean and std\n",
    "mean = norm_tensors['normalization_mean']\n",
    "std = norm_tensors['normalization_std']\n",
    "\n",
    "# Load a QR code detector\n",
    "detector = cv.QRCodeDetector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S7qVnO9B8IK-",
   "metadata": {
    "id": "S7qVnO9B8IK-"
   },
   "source": [
    "## Run the image processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "FDWEvRgsy_qC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7753,
     "status": "ok",
     "timestamp": 1654802471328,
     "user": {
      "displayName": "Jeffrey Neyhart",
      "userId": "07720342469706758133"
     },
     "user_tz": 240
    },
    "id": "FDWEvRgsy_qC",
    "outputId": "0e6fbec1-928c-4d1c-aff2-6b8e700fa576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images found.\n",
      "Segmentation complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Iterate over images and run through the prediction model ##\n",
    "\n",
    "# Rename the directory containing the images to segment\n",
    "dir_name = inference_directory\n",
    "filenames = os.listdir(dir_name)\n",
    "print(str(len(filenames)) + \" images found.\")\n",
    "\n",
    "# Create an empty list to store region data\n",
    "all_region_df = []\n",
    "\n",
    "# Iterate over the images\n",
    "for i, filename in enumerate(filenames):\n",
    "\n",
    "# ## TESTING ##\n",
    "# i = 0\n",
    "# filename = filenames[i]\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(dir_name +'/'+ filename)\n",
    "    image_cv = cv.imread(dir_name + \"/\" + filename)\n",
    "\n",
    "    ## Find the QR code and determine the sample name ##\n",
    "\n",
    "    # Crop the cv2 image\n",
    "    h, w, d = image_cv.shape\n",
    "    # Crop the image - this will look at the bottom-right corner\n",
    "    start_h = int(h / 2)\n",
    "    start_w = int(w / 2)\n",
    "    image_cv_crop = image_cv[start_h:h, start_w:w, :]\n",
    "    # Run the QR detector\n",
    "    collection_id, points, _ = detector.detectAndDecode(image_cv_crop)\n",
    "\n",
    "    ##\n",
    "\n",
    "    ## Find the color checker card\n",
    "    # Crop\n",
    "    image_cv_crop = image_cv[:, start_w:w, :]\n",
    "    h, w, d = image_cv_crop.shape\n",
    "    # Downsize\n",
    "    scale_percent = 25 # percent of original size\n",
    "    new_h = int(h * scale_percent / 100)\n",
    "    new_w = int(w * scale_percent / 100)\n",
    "\n",
    "    # Find the color card\n",
    "    df1, start, space = pcv.transform.find_color_card(rgb_img = cv.resize(image_cv_crop, (new_w, new_h)))\n",
    "\n",
    "    # Calculate the average box width and height\n",
    "    box_w = np.mean(df1['width'])\n",
    "    box_h = np.mean(df1['height'])\n",
    "    # Rescale the box width/height; this is the average full-scale box_dim\n",
    "    avg_box_dim = np.mean([x / (scale_percent / 100) for x in [box_w, box_h]])\n",
    "    # We know boxes are about 1.1581 cm on each side (square this to get area)\n",
    "    # Calculate the number of pixel per cm\n",
    "    pixel_per_cm = avg_box_dim / 1.1581\n",
    "    cm_per_pixel = 1 / pixel_per_cm\n",
    "\n",
    "    ##\n",
    "\n",
    "    # Convert the original image to grayscale\n",
    "    image_gray = np.asarray(image)\n",
    "    image_gray = rgb2gray(image_gray)\n",
    "    # Rescale\n",
    "    image_gray = rescale(image_gray, scale, anti_aliasing=True)\n",
    "\n",
    "    # Rescale the image\n",
    "    # h, w, d = image.shape\n",
    "    w, h = image.size\n",
    "    newW, newH = int(scale * w), int(scale * h)\n",
    "    # Convert image from nparray to pil image; remember to convert colors\n",
    "    # image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    # image = Image.fromarray(image)\n",
    "    image = image.resize((newW, newH))\n",
    "    image_gray = image_gray[0:newH, 0:newW] # Add this to resize image1\n",
    "    image = np.array(image, dtype = float)\n",
    "    new_im = np.zeros((3, newH, newW))\n",
    "    new_im[0,:,:] = image[:,:,0]\n",
    "    new_im[1,:,:] = image[:,:,1]\n",
    "    new_im[2,:,:] = image[:,:,2]\n",
    "    image_tensor = new_im\n",
    "\n",
    "    # Recalculate the pixels per cm\n",
    "    pixel_per_cm_scale = pixel_per_cm * scale\n",
    "    cm_per_pixel_scale = 1 / pixel_per_cm_scale\n",
    "    cm2_per_pixel_scale = cm_per_pixel_scale ** 2\n",
    "\n",
    "    # Create a tensor from the image\n",
    "    image_tensor = torch.from_numpy(image_tensor)\n",
    "    # Normalize the tensor and send it to the GPU\n",
    "    image_tensor = T.Normalize(mean=mean, std=std)(image_tensor)\n",
    "    image_tensor.unsqueeze_(0)\n",
    "    image_tensor = image_tensor.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    # Run the image through the prediction model\n",
    "    with torch.no_grad():\n",
    "        mask = model(image_tensor)['out']\n",
    "        mask = nn.Sigmoid()(mask)\n",
    "        mask = mask.cpu().detach().numpy()\n",
    "\n",
    "    # Iterate over materials to print\n",
    "    for mat_to_print in materials_toprint:\n",
    "        # Find the index of this material in the materials list\n",
    "        mat_idx = [i for i, x in enumerate(materials) if x.name == mat_to_print][0]\n",
    "\n",
    "        # Get the material at this index\n",
    "        mat = materials[mat_idx]\n",
    "\n",
    "        # Get the mask from the prediction model at this index\n",
    "        mat_mask = mask[0,mat_idx,:,:]\n",
    "        mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
    "        mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
    "\n",
    "        # Perform object segmentation and regionprop calculation\n",
    "        # This is from https://github.com/danforthcenter/plantcv/blob/master/plantcv/plantcv/watershed.py\n",
    "        # Convert the mat_mask to 8-bit\n",
    "        mat_mask = mat_mask.astype(\"uint8\")\n",
    "\n",
    "        # Run the watershed if called\n",
    "        if run_watershed:\n",
    "            # Run distance transform\n",
    "            dist_transform = cv.distanceTransformWithLabels(mat_mask, distanceType = cv.DIST_L2, maskSize = 0)[0]\n",
    "            local_max = feature.peak_local_max(dist_transform, indices = False, min_distance = distance, labels = mat_mask)\n",
    "\n",
    "            markers = ndi.label(local_max, structure=np.ones((3, 3)))[0]\n",
    "            dist_transform1 = -dist_transform\n",
    "            seg1 = segmentation.watershed(dist_transform1, markers, mask = mat_mask)\n",
    "        else:\n",
    "            seg1 = mat_mask\n",
    "\n",
    "        ## Estimate berry traits\n",
    "        # Label the segmentation output\n",
    "        label_mat = label(np.array(seg1), background = 0)\n",
    "\n",
    "        # Regionprops\n",
    "        region_properties1 = list(set([\"label\", \"bbox\"] + region_properties))\n",
    "        region_properties_names = region_properties1 + [x + \"_intensity_mean\" for x in [\"red\", \"green\", \"blue\"]] + [x + \"_intensity_sd\" for x in [\"red\", \"green\", \"blue\"]]\n",
    "        region_properties_names = region_properties_names + [x + \"_intensity_mean\" for x in [\"hue\", \"sat\", \"val\"]] + [x + \"_intensity_sd\" for x in [\"hue\", \"sat\", \"val\"]]\n",
    "        region_properties_names = tuple([\"unique_id\", \"material\", \"label\"] + [x for x in region_properties_names if x != \"label\"])\n",
    "\n",
    "        # Empty dictionary to store data\n",
    "        regions_dict = {}\n",
    "\n",
    "        # Initialize lists in the dictionary\n",
    "        for key in region_properties_names:\n",
    "            regions_dict[key] = []\n",
    "\n",
    "        # Iterate over regions in the image\n",
    "        for region in regionprops(label_image = label_mat):\n",
    "\n",
    "            # Add manual keys\n",
    "            regions_dict[\"unique_id\"] = collection_id\n",
    "            regions_dict[\"material\"] = mat_to_print\n",
    "\n",
    "            # Add props to the dictionary\n",
    "            for prop in region_properties1:\n",
    "                # If the property is a length, convert\n",
    "                if prop in [\"minor_axis_length\", \"major_axis_length\"]:\n",
    "                    to_append = region[prop] * cm_per_pixel_scale\n",
    "                elif prop == \"area\":\n",
    "                    to_append = region[prop] * cm2_per_pixel_scale\n",
    "                else:\n",
    "                    to_append = region[prop]\n",
    "\n",
    "                regions_dict[prop].append(to_append)\n",
    "\n",
    "\n",
    "            # Convert image to HSV\n",
    "            image_hsv = cv.cvtColor(image.astype(\"uint8\"), cv.COLOR_RGB2HSV)\n",
    "\n",
    "            berry_rgb_values = []\n",
    "            berry_hsv_values = []\n",
    "\n",
    "            for y, x in region.coords:\n",
    "                berry_rgb_values.append(image[y, x, :])\n",
    "                berry_hsv_values.append(image_hsv[y, x, :])\n",
    "\n",
    "            for c, left in enumerate([\"red\", \"green\", \"blue\"]):\n",
    "                key = left + \"_intensity_mean\"\n",
    "                vals = [x[c] for x in berry_rgb_values]\n",
    "                regions_dict[key].append(np.mean(vals))\n",
    "\n",
    "                key = key.replace(\"mean\", \"sd\")\n",
    "                regions_dict[key].append(np.std(vals))\n",
    "\n",
    "            for c, left in enumerate([\"hue\", \"sat\", \"val\"]):\n",
    "                key = left + \"_intensity_mean\"\n",
    "                vals = [x[c] for x in berry_hsv_values]\n",
    "                regions_dict[key].append(np.mean(vals))\n",
    "\n",
    "                key = key.replace(\"mean\", \"sd\")\n",
    "                regions_dict[key].append(np.std(vals))\n",
    "\n",
    "        # Convert the regions_dict to a data.frame\n",
    "        regions_df = pd.DataFrame(regions_dict)\n",
    "\n",
    "        # Remove excessively large regions\n",
    "        regions_df1 = regions_df[regions_df[\"area\"] <= 5]\n",
    "        \n",
    "        # Save the region data\n",
    "        all_region_df.append(regions_df1)\n",
    "\n",
    "        ### Save a segmentation image ###\n",
    "        if save_segmentation_image:\n",
    "            image_use = image.astype(\"uint8\")\n",
    "            regions_df_use = regions_df1.to_dict()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.imshow(image_use)\n",
    "\n",
    "            # Iterate over the berry index\n",
    "            for lab in regions_df_use[\"label\"]:\n",
    "                bbox = regions_df_use[\"bbox\"][lab]\n",
    "                # draw rectangle around segmented coins\n",
    "                minr, minc, maxr, maxc = bbox\n",
    "                rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                        fill=False, edgecolor='red', linewidth=2)\n",
    "                ax.add_patch(rect)\n",
    "                # Add label\n",
    "                ax.text(maxc, maxr, lab)\n",
    "\n",
    "            ax.set_axis_off()\n",
    "            plt.close(fig)\n",
    "            plt.savefig(seg_output_directory + \"/\" + collection_id + \"-\" + mat_to_print + \"-\" + filename)\n",
    "            \n",
    "\n",
    "# Save the region data\n",
    "# Merge the region data.feames\n",
    "all_regions_data = pd.concat(all_region_df)\n",
    "region_filename = output_directory + \"/\" + current_model_name + \"_InferenceImageRegionData.csv\"\n",
    "all_regions_data.to_csv(region_filename)\n",
    "            \n",
    "print(\"Segmentation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kksCPMtI-TPi",
   "metadata": {
    "id": "kksCPMtI-TPi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#############################\n",
    "#### Parameter  loading   ####\n",
    "#############################\n",
    " \n",
    "        \n",
    "dataset = BasicDataset(training_image_directory, training_mask_directory, scale=scale, transform=False)\n",
    " \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!Set batch size here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# train, val=trainval_split(dataset, val_fraction=0.5)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    "#val_loader = DataLoader(val, batch_size=3, shuffle=False, num_workers=0, pin_memory=True)#, collate_fn=pad_collate)\n",
    "nimages = 0\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for batch, _ in train_loader:\n",
    "    # Rearrange batch to be the shape of [B, C, W * H]\n",
    "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "    # Update total number of images\n",
    "    nimages += batch.size(0)\n",
    "    # Compute mean and std here\n",
    "    mean += batch.mean(2).sum(0) \n",
    "    std += batch.std(2).sum(0)\n",
    " \n",
    "# Final step\n",
    "mean /= nimages\n",
    "std /= nimages\n",
    " \n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "dataset.means=mean\n",
    "dataset.stds=std \n",
    "\n",
    "nimages = 0\n",
    "newmean = 0.\n",
    "newstd = 0.\n",
    "for batch, _ in train_loader:\n",
    "    # Rearrange batch to be the shape of [B, C, W * H]\n",
    "    batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "    # Update total number of images\n",
    "    nimages += batch.size(0)\n",
    "    # Compute mean and std here\n",
    "    newmean += batch.mean(2).sum(0) \n",
    "    newstd += batch.std(2).sum(0)\n",
    " \n",
    "# Final step\n",
    "newmean /= nimages\n",
    "newstd /= nimages\n",
    " \n",
    "print(newmean)\n",
    "print(newstd)\n",
    "\n",
    "\n",
    "## Save these normalization values for the production pipeline ##\n",
    "\n",
    "# Open a file\n",
    "param_filename = model_group_directory + model_group.replace(\"/\", \"\") + \"_normalization_param.txt\"\n",
    "handle = open(param_filename, \"w\")\n",
    "\n",
    "# Write all the parameters to this file\n",
    "handle.write(\"normalization_mean\" + \"\\t\" + str(newmean) + \"\\n\")\n",
    "handle.write(\"normalization_std\" + \"\\t\" + str(newstd) + \"\\n\")\n",
    "\n",
    "# Close the file\n",
    "handle.close()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "80899b8d-4327-4898-81b2-9a8bd430302c",
    "OaEzTdfG-WuR"
   ],
   "name": "Deploy_BerryBox_FCNSegmentationModel_Developmental_Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "virtenv_cuda113",
   "language": "python",
   "name": "virtenv_cuda113"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
